{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwFJNPKj_c5L"
      },
      "source": [
        "> **Note:** Use the provided YAML config exactly as-is to replicate reported results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:23:15.398987Z",
          "iopub.status.busy": "2026-01-28T20:23:15.3988Z",
          "iopub.status.idle": "2026-01-28T20:23:40.177966Z",
          "shell.execute_reply": "2026-01-28T20:23:40.177236Z",
          "shell.execute_reply.started": "2026-01-28T20:23:15.398966Z"
        },
        "id": "GrF0zuUY_FRc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q -U kagglehub torchio wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ciwg_JSB_ilk"
      },
      "source": [
        "## Imports & Global configs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:23:40.180182Z",
          "iopub.status.busy": "2026-01-28T20:23:40.179909Z",
          "iopub.status.idle": "2026-01-28T20:23:47.78077Z",
          "shell.execute_reply": "2026-01-28T20:23:47.780065Z",
          "shell.execute_reply.started": "2026-01-28T20:23:40.180155Z"
        },
        "id": "_iJ2XS-661jc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import List, Literal\n",
        "\n",
        "import kagglehub\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchio as tio\n",
        "import wandb\n",
        "import yaml\n",
        "\n",
        "from collections import defaultdict\n",
        "from pprint import pprint\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from typing import Optional\n",
        "\n",
        "awsaf49_brats20_dataset_training_validation_path = kagglehub.dataset_download('awsaf49/brats20-dataset-training-validation')\n",
        "print('Data source import complete.')\n",
        "print(\"Path to dataset files:\", awsaf49_brats20_dataset_training_validation_path)\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "config = load_config('./config_unet.yaml')\n",
        "config['hardware']['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "pprint(config, indent=4, width=80, compact=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:23:47.782393Z",
          "iopub.status.busy": "2026-01-28T20:23:47.781886Z",
          "iopub.status.idle": "2026-01-28T20:23:47.792104Z",
          "shell.execute_reply": "2026-01-28T20:23:47.791437Z",
          "shell.execute_reply.started": "2026-01-28T20:23:47.782365Z"
        },
        "id": "jTZJq5Jy4WhH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def _center_crop(volume: torch.Tensor, target_shape: tuple) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Center-crops a 3D volume to the target shape in (H, W, D) order.\n",
        "\n",
        "    Parameters:\n",
        "        volume (torch.Tensor): Input volume of shape (..., H, W, D) or similar\n",
        "        target_shape (tuple): Target shape as (tH, tW, tD)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Cropped volume\n",
        "    \"\"\"\n",
        "    h, w, d = volume.shape[-3:]\n",
        "    tH, tW, tD = target_shape\n",
        "\n",
        "    h_start = (h - tH) // 2\n",
        "    w_start = (w - tW) // 2\n",
        "    d_start = (d - tD) // 2\n",
        "\n",
        "    return volume[...,\n",
        "                  h_start:h_start+tH,\n",
        "                  w_start:w_start+tW,\n",
        "                  d_start:d_start+tD]\n",
        "\n",
        "def apply_augmentation(\n",
        "    multimodal_tensor: torch.Tensor,\n",
        "    segmentation_tensor: torch.Tensor = None, # Made optional for Autoencoder use\n",
        "    modalities_list = ['t1', 't1ce', 't2', 'flair'],\n",
        "    geometric_transforms=None,\n",
        "    intensity_transforms=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Apply augmentation pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (transformed_multimodal, transformed_segmentation, foreground_mask)\n",
        "    \"\"\"\n",
        "\n",
        "    # Create TorchIO Subject\n",
        "    subject_dict = {}\n",
        "    for i, mod_name in enumerate(modalities_list):\n",
        "        subject_dict[mod_name] = tio.ScalarImage(tensor=multimodal_tensor[i:i+1]) # Slicing instead of indexing to preserver dim=0\n",
        "\n",
        "    # Only add segmentation to subject if it is provided\n",
        "    if segmentation_tensor is not None:\n",
        "        subject_dict['seg'] = tio.LabelMap(tensor=segmentation_tensor.unsqueeze(0))\n",
        "\n",
        "    subject = tio.Subject(**subject_dict)\n",
        "\n",
        "    # Apply geometric transforms\n",
        "    if geometric_transforms is not None:\n",
        "        subject = geometric_transforms(subject)\n",
        "\n",
        "    # Extract foreground mask after geometric transforms\n",
        "    # This is the important mask we will use for everything else.\n",
        "    mask = None\n",
        "    if intensity_transforms is not None:\n",
        "        # We can create the mask from any of the modalities, BraTS MRIS are co-registered.\n",
        "        for mod_name in modalities_list:\n",
        "            mask = subject[mod_name].data > 0\n",
        "            break\n",
        "\n",
        "    # Apply intensity transforms\n",
        "    if intensity_transforms is not None and mask is not None:\n",
        "        subject = intensity_transforms(subject)\n",
        "\n",
        "        # Reapply mask to zero out background\n",
        "        for mod_name in modalities_list:\n",
        "            # subject[mod_name].data = subject[mod_name].data * mask  [DeprecationWarning]\n",
        "            new_data = subject[mod_name].data * mask\n",
        "            subject[mod_name].set_data(new_data)\n",
        "\n",
        "    # Extract transformed tensors\n",
        "    multimodal_list = [subject[mod].data for mod in modalities_list]\n",
        "    transformed_multimodal = torch.cat(multimodal_list, dim=0)\n",
        "\n",
        "    # Handle the optional segmentation extraction\n",
        "    transformed_segmentation = None\n",
        "    if 'seg' in subject:\n",
        "        transformed_segmentation = subject['seg'].data.squeeze(0)\n",
        "\n",
        "    # Return the generated mask along with the tensors\n",
        "    # If no intensity transforms were applied, the mask will be None.\n",
        "    if mask is None:\n",
        "        mask = transformed_multimodal > 0\n",
        "\n",
        "    return transformed_multimodal, transformed_segmentation, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOA0TzaD61jg"
      },
      "source": [
        "## Dataset Definition\n",
        "### Helpful infos\n",
        "\n",
        "The pipeline always downloads the **original BraTS2020 dataset** from Kaggle as the primary data source.\n",
        "\n",
        "**Segmentation loading modes (`seg_mode`):**\n",
        "\n",
        "The `seg_mode` parameter controls how segmentation masks are loaded, offering a trade-off between speed and storage:\n",
        "\n",
        "- **`\"raw\"`** (slowest, minimal storage):\n",
        "  - Loads compressed NIfTI segmentation masks (`.nii`) directly from the original dataset\n",
        "  - No preprocessing directory required\n",
        "\n",
        "- **`\"preprocessed\"`** (fastest, requires storage):\n",
        "  - Loads pre-generated uncompressed `.npy` segmentation masks from `preprocessed_root_dir`\n",
        "  - If preprocessed files are missing or incomplete, automatically triggers preprocessing\n",
        "  - Preprocessing: converts `.nii` to `.npy`, applies label remapping, and optionally generates `assign_label5`\n",
        "\n",
        "- **`\"force_generate\"`** (for updates):\n",
        "  - Regenerates `.npy` masks, even if they already exist in `preprocessed_root_dir`\n",
        "  - Use this when preprocessing logic changes (e.g., toggling `assign_label5`)\n",
        "  - Training-wise, identical to `preprocessed`\n",
        "\n",
        "**Preprocessing Directory:**\n",
        "- `preprocessed_root_dir` can be explicitly set or defaults to `<root_dir>/processed`\n",
        "- Stores only segmentation masks as `.npy` files (MRI images are always loaded from original `.nii` to save storage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:23:47.793813Z",
          "iopub.status.busy": "2026-01-28T20:23:47.793146Z",
          "iopub.status.idle": "2026-01-28T20:23:47.816656Z",
          "shell.execute_reply": "2026-01-28T20:23:47.816008Z",
          "shell.execute_reply.started": "2026-01-28T20:23:47.793775Z"
        },
        "id": "9OeG2Qcx61ji",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class BraTSDataset(Dataset):\n",
        "    \"\"\"\n",
        "    BraTS2020 Dataset for 3D multi-modal MRI brain tumor segmentation.\n",
        "    {Verbose docstring for clarity.}\n",
        "\n",
        "    Args:\n",
        "        split (str):\n",
        "            Dataset split to load. Must be either \"train\" or \"test\".\n",
        "            Augmentations are applied only when split=\"train\".\n",
        "\n",
        "        train_ratio (float):\n",
        "            Fraction of the dataset used for training. The remainder is used\n",
        "            for testing after shuffling with a fixed random seed.\n",
        "\n",
        "        root_dir (str):\n",
        "            Root directory containing the BraTS2020 dataset. Required.\n",
        "\n",
        "        preprocessed_root_dir (Optional[str]):\n",
        "            Root directory used to store and/or load preprocessed segmentation\n",
        "            masks saved as .npy files. If None, defaults to '<root_dir>/processed'.\n",
        "\n",
        "        modalities_list (List[str]):\n",
        "            List of MRI modalities to load for each subject (e.g., [\"t1\",\n",
        "            \"t1ce\", \"t2\", \"flair\"]). The order defines the channel order in\n",
        "            the output tensor.\n",
        "\n",
        "        seg_mode (Literal[\"raw\", \"preprocessed\", \"force_generate\"]):\n",
        "            Controls how segmentation masks are loaded:\n",
        "                1) \"raw\": Load original compressed NIfTI segmentation masks [slowest].\n",
        "                2) \"preprocessed\": Load pre-generated .npy segmentation masks [fastest].\n",
        "                  If the preprocessing directory is missing or empty, all\n",
        "                  masks are generated and locally stored.\n",
        "                3) \"force_generate\": Always regenerate .npy segmentation masks\n",
        "                  before loading (even if preprocessed_root_dir is not empty).\n",
        "                  [Required if preprocessing logic is updated.]\n",
        "\n",
        "        assign_label5 (bool):\n",
        "            If True, during preprocessing assigns a separate label to non-tumor\n",
        "            brain tissue. It is referred to as \"label 5\" to follow BraTS\n",
        "            conventions (labels {0,1,2,4}), but after remapping (4 >> 3) it is\n",
        "            stored internally as label 4.\n",
        "            [assign_label5 in \"raw\" mode is not supported.]\n",
        "\n",
        "        geometric_transforms:\n",
        "            TorchIO spatial transforms applied jointly to image and\n",
        "            segmentation volumes during training. Applied before cropping.\n",
        "\n",
        "        intensity_transforms:\n",
        "            TorchIO intensity transforms applied to image volumes during\n",
        "            training. Applied after cropping.\n",
        "\n",
        "        output_shape (Tuple[int, int, int]):\n",
        "            Target spatial shape (H, W, D) for center cropping of images,\n",
        "            segmentations, and foreground masks.\n",
        "\n",
        "        norm_type (str):\n",
        "            Intensity normalization strategy. Supported values are:\n",
        "                - \"z\": Z-normalization within the foreground mask.\n",
        "                - \"minmax\": Min-max normalization.\n",
        "\n",
        "        random_seed (int):\n",
        "            Random seed used for reproducible shuffling and train/test\n",
        "            splitting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: str = \"train\",\n",
        "        train_ratio: float = 0.8,\n",
        "        root_dir: str = \"../kaggle/input/brats20-dataset-training-validation/\",\n",
        "        preprocessed_root_dir: Optional[str] = None,\n",
        "        modalities_list: List[str] = [\"t1\", \"t1ce\", \"t2\", \"flair\"],\n",
        "        seg_mode: Literal[\"raw\", \"preprocessed\", \"force_generate\"] = \"raw\",\n",
        "        assign_label5: bool = True,\n",
        "        geometric_transforms=None,\n",
        "        intensity_transforms=None,\n",
        "        output_shape: tuple = (128, 128, 128),\n",
        "        norm_type: str = \"z\",\n",
        "        random_seed: int = 42,\n",
        "    ):\n",
        "        self.split = split\n",
        "        self.modalities_list = modalities_list\n",
        "        self.seg_mode = seg_mode\n",
        "        self.assign_label5 = assign_label5\n",
        "        self.geometric_transforms = geometric_transforms\n",
        "        self.intensity_transforms = intensity_transforms\n",
        "        self.output_shape = output_shape\n",
        "        self.norm_type = norm_type\n",
        "\n",
        "        if root_dir is None:\n",
        "            raise ValueError(\"root_dir must be provided\")\n",
        "\n",
        "        self.root_dir = Path(root_dir)\n",
        "\n",
        "        self.preprocessed_root = (\n",
        "            Path(preprocessed_root_dir)\n",
        "            if preprocessed_root_dir is not None\n",
        "            else self.root_dir / \"processed\"\n",
        "        )\n",
        "\n",
        "        self.samples_path = ( # Default path\n",
        "            self.root_dir\n",
        "            / \"BraTS2020_TrainingData\"\n",
        "            / \"MICCAI_BraTS2020_TrainingData\"\n",
        "        )\n",
        "\n",
        "        if not self.samples_path.exists():\n",
        "            raise FileNotFoundError(f\"Raw data not found at {self.samples_path}\")\n",
        "\n",
        "        self.preprocessed_path = ( # Preprocessed path, this is either passed or built from the default path\n",
        "            self.preprocessed_root\n",
        "            / \"BraTS2020_TrainingData\"\n",
        "            / \"MICCAI_BraTS2020_TrainingData\"\n",
        "        )\n",
        "\n",
        "        total_samples = 369\n",
        "        all_indices = list(range(1, total_samples + 1))\n",
        "        random.seed(random_seed)\n",
        "        random.shuffle(all_indices)\n",
        "\n",
        "        split_idx = int(train_ratio * total_samples)\n",
        "        if split == \"train\":\n",
        "            self.sample_indices = all_indices[:split_idx]\n",
        "        elif split == \"test\":\n",
        "            self.sample_indices = all_indices[split_idx:]\n",
        "        else:\n",
        "            raise ValueError(\"split must be 'train' or 'test'\")\n",
        "\n",
        "        if self.seg_mode == \"force_generate\":\n",
        "            print(\"Force generating preprocessed data...\")\n",
        "            self._preprocess_dataset()\n",
        "        elif self.seg_mode == \"preprocessed\":\n",
        "            # Check if all samples are preprocessed (count directories)\n",
        "            if not self.preprocessed_path.exists():\n",
        "                num_preprocessed = 0\n",
        "            else:\n",
        "                num_preprocessed = len([d for d in self.preprocessed_path.iterdir() if d.is_dir()])\n",
        "\n",
        "            if num_preprocessed < total_samples:\n",
        "                print(f\"Preprocessed files incomplete ({num_preprocessed}/{total_samples}). Generating now...\")\n",
        "                self._preprocess_dataset()\n",
        "            else:\n",
        "                print(f\"All preprocessed data verified ({total_samples} samples).\")\n",
        "\n",
        "        self.use_npy = (self.seg_mode in [\"preprocessed\", \"force_generate\"]) # Preprocessing works with .npy\n",
        "\n",
        "    def _preprocess_dataset(self):\n",
        "        \"\"\"\n",
        "        Converts NIfTI segmentations to uncompressed .npy for higher speed data loading.\n",
        "        Remaps labels (4 -> 3) and optionally generates non-tumor tissue [label 5 (label 4 after remapping)].\n",
        "        \"\"\"\n",
        "        self.preprocessed_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        for idx in tqdm(self.sample_indices, desc=\"Preprocessing\"):\n",
        "            sid = f\"{idx:03d}\"\n",
        "            name = f\"BraTS20_Training_{sid}\"\n",
        "            raw_path = self.samples_path / name\n",
        "            out_path = self.preprocessed_path / name\n",
        "            out_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "            if idx == 355:  # Handle known naming inconsistency in dataset [Colab doesn't allow to rename it.]\n",
        "                seg_path = raw_path / \"W39_1998.09.19_Segm.nii\"\n",
        "            else:\n",
        "                seg_path = raw_path / f\"{name}_seg.nii\"\n",
        "\n",
        "            seg = self._load_nifti_volume(seg_path)\n",
        "            seg[seg == 4] = 3  # Remap 4 >> 3\n",
        "\n",
        "            if self.assign_label5:\n",
        "                mri = self._load_nifti_volume(raw_path / f\"{name}_{self.modalities_list[0]}.nii\")\n",
        "                label4_mask = (seg == 0) & (mri != 0)\n",
        "                seg[label4_mask] = 4\n",
        "\n",
        "            np.save(out_path / f\"{name}_seg.npy\", seg)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx_val = self.sample_indices[idx]\n",
        "        sample_id = f\"{idx_val:03d}\"\n",
        "        sample_name = f\"BraTS20_Training_{sample_id}\"\n",
        "\n",
        "        raw_sample_path = self.samples_path / sample_name\n",
        "        prep_sample_path = self.preprocessed_path / sample_name\n",
        "\n",
        "        img = self._load_raw_mri(raw_sample_path, sample_name) # MRI is always loaded as .nii\n",
        "        # Applying the same pre-processing logic here would mean to save uncompressed\n",
        "        # 4-modalities 3D MRIs, fp32 precision, this blows memory.\n",
        "\n",
        "        # Unified logic: Load .npy if we are in 'preprocessed' or 'force_generate' mode\n",
        "        if self.use_npy:\n",
        "            seg_path = prep_sample_path / f\"{sample_name}_seg.npy\"\n",
        "            seg = torch.from_numpy(np.load(seg_path))\n",
        "        else:\n",
        "            # Fallback: Load original .nii segmentation\n",
        "            # Note: this is slower and inefficient. But requires much less storage.\n",
        "            if idx_val == 355:  # Handle known naming inconsistency in dataset [Colab doesn't allow to rename it.]\n",
        "                seg_path = raw_sample_path / \"W39_1998.09.19_Segm.nii\"\n",
        "            else:\n",
        "                seg_path = raw_sample_path / f\"{sample_name}_seg.nii\"\n",
        "            seg_np = self._load_nifti_volume(seg_path)\n",
        "            seg_np[seg_np == 4] = 3\n",
        "\n",
        "            if self.assign_label5:\n",
        "                mri_np = img[0].numpy()\n",
        "                label5_mask = (seg_np == 0) & (mri_np != 0)\n",
        "                seg_np[label5_mask] = 4\n",
        "            seg = torch.from_numpy(seg_np)\n",
        "            # To support assign_label5 in \"raw\" mode we could either\n",
        "            # 1) Perform it on the fly (as above^^^) -> Redundant,at every epoch we would be performing same operations on the same samples.\n",
        "            # 2) Pre-process it and overwrite the original .nii files -> Colab doesn't allow to modify those write-only files.\n",
        "            # 1 is implemented but acknowledge it's suboptimal.\n",
        "\n",
        "        foreground_mask = None\n",
        "        if self.split == \"train\" and (self.geometric_transforms or self.intensity_transforms):\n",
        "            img, seg, foreground_mask = apply_augmentation(\n",
        "                img, seg, self.modalities_list, self.geometric_transforms, self.intensity_transforms\n",
        "            )\n",
        "            foreground_mask = foreground_mask.squeeze(0)\n",
        "\n",
        "        if self.output_shape is not None:\n",
        "            img = _center_crop(img, self.output_shape)\n",
        "            seg = _center_crop(seg, self.output_shape)\n",
        "            if foreground_mask is not None:\n",
        "                foreground_mask = _center_crop(foreground_mask, self.output_shape)\n",
        "\n",
        "        if foreground_mask is None:\n",
        "            foreground_mask = img[0] > 0\n",
        "\n",
        "        img = self._normalize(img, foreground_mask)\n",
        "        return img, seg\n",
        "\n",
        "    def _load_raw_mri(self, sample_path: Path, sample_name: str) -> torch.Tensor:\n",
        "        volumes = []\n",
        "        for mod in self.modalities_list:\n",
        "            path = sample_path / f\"{sample_name}_{mod}.nii\"\n",
        "            volumes.append(self._load_nifti_volume(path))\n",
        "        return torch.from_numpy(np.stack(volumes, axis=0))\n",
        "\n",
        "    def _normalize(self, tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "        if self.norm_type == \"z\":\n",
        "            for c in range(tensor.shape[0]):\n",
        "                if mask.any():\n",
        "                    mean = tensor[c][mask].mean()\n",
        "                    std = tensor[c][mask].std()\n",
        "                    if std > 0:\n",
        "                        tensor[c][mask] = (tensor[c][mask] - mean) / std\n",
        "        elif self.norm_type == \"minmax\":\n",
        "            subject = tio.Subject(image=tio.ScalarImage(tensor=tensor))\n",
        "            transform = tio.RescaleIntensity(\n",
        "                out_min_max=(0, 1),\n",
        "                percentiles=(0.0, 99.5),\n",
        "            )\n",
        "            tensor = transform(subject).image.data\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown norm_type: {self.norm_type}\")\n",
        "        return tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_nifti_volume(path: Path) -> np.ndarray:\n",
        "        nii = nib.load(str(path))\n",
        "        return np.asarray(nii.dataobj, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFdrJmX5jJAa"
      },
      "source": [
        "## Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:23:47.817657Z",
          "iopub.status.busy": "2026-01-28T20:23:47.817397Z",
          "iopub.status.idle": "2026-01-28T20:23:47.83838Z",
          "shell.execute_reply": "2026-01-28T20:23:47.837596Z",
          "shell.execute_reply.started": "2026-01-28T20:23:47.817627Z"
        },
        "id": "ZXT-8AIkO5jS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(\n",
        "    train_ratio: float = 0.8,\n",
        "    train_batch_size: int = 1,\n",
        "    test_batch_size: int = 1,\n",
        "    geometric_transforms=None,\n",
        "    intensity_transforms=None,\n",
        "    output_shape: tuple = (128, 128, 128),\n",
        "    norm_type: str = \"z\",\n",
        "    seed: int = 42,\n",
        "    root_dir: str = \"./\",\n",
        "    preprocessed_root_dir: str | None = None,\n",
        "    seg_mode: Literal[\"raw\", \"preprocessed\", \"force_generate\"] = \"raw\",\n",
        "    assign_label5: bool = False,\n",
        "    num_workers: int = 0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Create train and test dataloaders for BraTS20Dataset.\n",
        "\n",
        "    Returns:\n",
        "        train_dataloader, test_dataloader\n",
        "    \"\"\"\n",
        "\n",
        "    train_dataset = BraTSDataset(\n",
        "        split=\"train\",\n",
        "        train_ratio=train_ratio,\n",
        "        root_dir=root_dir,\n",
        "        preprocessed_root_dir=preprocessed_root_dir,\n",
        "        seg_mode=seg_mode,\n",
        "        assign_label5=assign_label5,\n",
        "        geometric_transforms=geometric_transforms,\n",
        "        intensity_transforms=intensity_transforms,\n",
        "        output_shape=output_shape,\n",
        "        norm_type=norm_type,\n",
        "        random_seed=seed,\n",
        "    )\n",
        "\n",
        "    test_dataset = BraTSDataset(\n",
        "        split=\"test\",\n",
        "        train_ratio=train_ratio,\n",
        "        root_dir=root_dir,\n",
        "        preprocessed_root_dir=preprocessed_root_dir,\n",
        "        seg_mode=seg_mode,\n",
        "        assign_label5=assign_label5,\n",
        "        geometric_transforms=None,\n",
        "        intensity_transforms=None,\n",
        "        output_shape=output_shape,\n",
        "        norm_type=norm_type,\n",
        "        random_seed=seed,\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTrain dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=test_batch_size,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    return train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:23:47.839772Z",
          "iopub.status.busy": "2026-01-28T20:23:47.839471Z",
          "iopub.status.idle": "2026-01-28T20:26:08.25815Z",
          "shell.execute_reply": "2026-01-28T20:26:08.257352Z",
          "shell.execute_reply.started": "2026-01-28T20:23:47.839742Z"
        },
        "id": "2X19TVF5EfA3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "geometric_transform = tio.Compose([\n",
        "    tio.RandomFlip(axes=('LR',), flip_probability=0.5),\n",
        "    tio.RandomAffine(\n",
        "    scales=(0.6, 1.4),\n",
        "    degrees=(15, 15, 15),\n",
        "    translation=(5, 5, 5),\n",
        "    isotropic=False,\n",
        "    p=0.35\n",
        ")\n",
        "])\n",
        "\n",
        "intensity_transform = tio.Compose([\n",
        "    tio.RandomGamma(log_gamma=(-0.3, 0.3), p=0.3),\n",
        "])\n",
        "\n",
        "data_cfg = config['data']\n",
        "train_cfg = config['training']\n",
        "\n",
        "train_dataloader, test_dataloader = get_dataloaders(\n",
        "    train_ratio=data_cfg[\"train_ratio\"],\n",
        "    train_batch_size=train_cfg[\"train_batch_size\"],\n",
        "    test_batch_size=train_cfg[\"test_batch_size\"],\n",
        "    output_shape=tuple(data_cfg[\"output_shape\"]),  # YAML list > tuple\n",
        "    norm_type=data_cfg[\"norm_type\"],\n",
        "    root_dir=awsaf49_brats20_dataset_training_validation_path,\n",
        "    preprocessed_root_dir=data_cfg[\"preprocessed_dir\"],\n",
        "    seg_mode=data_cfg[\"seg_mode\"],\n",
        "    assign_label5=data_cfg[\"assign_label5\"],\n",
        "    geometric_transforms=geometric_transform,\n",
        "    intensity_transforms=intensity_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxW00j9NIn3F"
      },
      "source": [
        "## Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:26:13.826682Z",
          "iopub.status.busy": "2026-01-28T20:26:13.826215Z",
          "iopub.status.idle": "2026-01-28T20:26:13.852073Z",
          "shell.execute_reply": "2026-01-28T20:26:13.851482Z",
          "shell.execute_reply.started": "2026-01-28T20:26:13.826657Z"
        },
        "id": "khUvbgUCWywH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class SEBlock3d(nn.Module):\n",
        "    def __init__(self, channels, reduction=8, min_bottleneck=4):\n",
        "        super().__init__()\n",
        "        bottleneck = max(channels // reduction, min_bottleneck)\n",
        "\n",
        "        self.squeeze = nn.AdaptiveAvgPool3d(1) # (B, C, 1, 1, 1)\n",
        "\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(channels, bottleneck, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(bottleneck, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _, _ = x.shape\n",
        "        y = self.squeeze(x).view(b, c) # view squeezes out the 1,1,1\n",
        "        y = self.excitation(y).view(b, c, 1, 1, 1) #  view puts back the 1,1,1 to align to x.shape\n",
        "        return x * y\n",
        "\n",
        "class nConvBlock3d(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual 3D convolutional block with N stacked convolutions.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of input channels.\n",
        "        out_channels (int): Number of output channels.\n",
        "        kernel_size (int): Size of the 3D convolution kernel. Default is 3.\n",
        "        n_convs (int): Number of convolutional layers in the block.\n",
        "        use_se (bool): Whether to apply a Squeeze-and-Excitation block.\n",
        "        mid_channels (int, optional): Number of channels used for intermediate\n",
        "            convolutions. Defaults to out_channels. This allows a, optional,\n",
        "            gradual transition from in_channels to out_channels.\n",
        "\n",
        "    Output:\n",
        "        Tensor: Output tensor of shape (B, out_channels, H, W, D).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, n_convs=2, use_se=False, mid_channels=None):\n",
        "        super().__init__()\n",
        "        self.n_convs = n_convs\n",
        "\n",
        "        # If mid_channels not provided, use out_channels\n",
        "        if mid_channels is None:\n",
        "            mid_channels = out_channels\n",
        "\n",
        "        layers = []\n",
        "        for i in range(n_convs):\n",
        "            in_ch = in_channels if i == 0 else mid_channels\n",
        "            out_ch = out_channels if i == n_convs - 1 else mid_channels\n",
        "            layers.append(nn.Conv3d(in_ch, out_ch, kernel_size=kernel_size, padding='same'))\n",
        "            layers.append(nn.GroupNorm(num_groups=8, num_channels=out_ch))\n",
        "            layers.append(nn.LeakyReLU(0.01, inplace=True))\n",
        "\n",
        "        self.conv_sequence = nn.Sequential(*layers)\n",
        "        self.se = SEBlock3d(out_channels) if use_se else nn.Identity()\n",
        "\n",
        "    def forward(self, x, custom_skip_tensor=None):\n",
        "        if custom_skip_tensor is not None:\n",
        "            skip_connection = custom_skip_tensor\n",
        "        else:\n",
        "            skip_connection = x\n",
        "        x = self.conv_sequence(x)\n",
        "        x = self.se(x)\n",
        "\n",
        "        return x + skip_connection\n",
        "\n",
        "class Down3d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample_mode=None):\n",
        "        super(Down3d, self).__init__()\n",
        "\n",
        "        if downsample_mode:\n",
        "            if downsample_mode == 'conv': # Learnable down-sampling\n",
        "                self.downsample = nn.Sequential(\n",
        "                    nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "                    nn.GroupNorm(num_groups=8, num_channels=out_channels),\n",
        "                    nn.LeakyReLU(0.01, inplace=True)\n",
        "                )\n",
        "            elif downsample_mode == 'pool':\n",
        "                # Downsample with pooling + 1x1 conv.\n",
        "                self.downsample = nn.Sequential(\n",
        "                    nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "                    nn.Conv3d(in_channels, out_channels, kernel_size=1),\n",
        "                    nn.GroupNorm(num_groups=8, num_channels=out_channels),\n",
        "                    nn.LeakyReLU(0.01, inplace=True)\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown downsample_mode: {downsample_mode}\")\n",
        "        else: # No spatial downsampling, used in the initial encoder for the chann expansion.\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv3d(in_channels, out_channels, kernel_size=1),\n",
        "                nn.GroupNorm(num_groups=8, num_channels=out_channels),\n",
        "                nn.LeakyReLU(0.01, inplace=True)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.downsample(x)\n",
        "\n",
        "\n",
        "class Up3d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=2, stride=2):\n",
        "        super().__init__()\n",
        "        self.upconv = nn.ConvTranspose3d(in_channels, out_channels,\n",
        "                                         kernel_size=kernel_size, stride=stride)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x_upsampled = self.upconv(x)\n",
        "\n",
        "        if x_upsampled.shape[2:] != skip.shape[2:]:\n",
        "            _, _, H, W, D = skip.shape\n",
        "            _, _, H_chunk, W_chunk, D_chunk = x_upsampled.shape\n",
        "            H_start = (H - H_chunk) // 2\n",
        "            W_start = (W - W_chunk) // 2\n",
        "            D_start = (D - D_chunk) // 2\n",
        "            skip = skip[:, :, H_start:H_start+H_chunk, W_start:W_start+W_chunk, D_start:D_start+D_chunk]\n",
        "\n",
        "        x_concatenated = torch.cat([x_upsampled, skip], dim=1)\n",
        "\n",
        "        return x_concatenated, x_upsampled\n",
        "\n",
        "\n",
        "class EncoderLayer3d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample_mode='conv', n_convs=2, use_skip_se=False, mid_channels=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample = Down3d(in_channels, out_channels, downsample_mode=downsample_mode)\n",
        "        self.n_convblock = nConvBlock3d(out_channels, out_channels, use_se=False, n_convs=n_convs, mid_channels=mid_channels) # use_se=False on the Encoder's path.\n",
        "        self.skip_se = SEBlock3d(out_channels) if use_skip_se else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_down = self.downsample(x)          # pre-SE features for downward path\n",
        "        x_down = self.n_convblock(x_down)\n",
        "        skip = self.skip_se(x_down)          # skip connection with SE if enabled\n",
        "        return x_down, skip\n",
        "\n",
        "class DecoderLayer3d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, n_convs=2, use_se=False, mid_channels=None):\n",
        "        super().__init__()\n",
        "        self.n_convs=n_convs\n",
        "        self.up = Up3d(in_channels, out_channels, kernel_size=2, stride=2) # Spatial upsampling and channel reduction.\n",
        "        self.n_convblock = nConvBlock3d(2*out_channels, out_channels, n_convs=n_convs, use_se=use_se, mid_channels=mid_channels)\n",
        "        # ^^^ 2*out_channels as input because we perform a concatenation with the skip first.\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x_concatenated, x_upsampled = self.up(x, skip)\n",
        "        return self.n_convblock(x_concatenated, custom_skip_tensor=x_upsampled)\n",
        "\n",
        "\n",
        "class cvNet2(nn.Module): # cv -> Colle Vincenzo (^_~)\n",
        "    \"\"\"\n",
        "    3D U-Net + SE + encoder/decoder input-output residuals.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=4, out_channels=4,\n",
        "                 features=[32, 64, 128, 256, 320],\n",
        "                 n_convs=[1, 2, 3, 3, 3],\n",
        "                 downsample_mode='conv',\n",
        "                 use_skip_se=True,\n",
        "                 use_decoder_se=True):\n",
        "        super().__init__()\n",
        "\n",
        "        skip_se_config = [use_skip_se and f >= 0 for f in features]  # >= n: tinkering, idea was to apply SE only for deeper layers.\n",
        "        # decoder_se_config = [use_decoder_se for f in features[:-1]]\n",
        "\n",
        "        if len(n_convs) < len(features):\n",
        "            n_convs = n_convs + [n_convs[-1]] * (len(features) - len(n_convs))\n",
        "        assert len(features) == len(n_convs)\n",
        "\n",
        "        encoder_n_convs = n_convs[1:] # exclude initial encoder layer\n",
        "        decoder_n_convs = encoder_n_convs[::-1][:-1] # mirror, remove last for final\n",
        "        # Example of ^^^\n",
        "        # n_convs = [1, 2, 3, 3, 3]\n",
        "        # encoder_n_convs = [2, 3, 3, 3] -> There is one more because it handles the bottleneck as well.\n",
        "        # decoder_n_convs = [3, 3, 2]\n",
        "\n",
        "        self.initial_encoder_layer = EncoderLayer3d(in_channels, features[0],\n",
        "                                        downsample_mode=None,\n",
        "                                        use_skip_se=skip_se_config[0],\n",
        "                                        n_convs=n_convs[0]) # No spatial downsampling\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            EncoderLayer3d(features[i], features[i+1],\n",
        "                    downsample_mode=downsample_mode,\n",
        "                    use_skip_se=skip_se_config[i+1], # i+1 since intial encoder layer is not defined here.\n",
        "                    n_convs=n_convs[i+1])\n",
        "            for i in range(len(features)-1)\n",
        "        ])\n",
        "\n",
        "        self.decoder = nn.ModuleList([\n",
        "            DecoderLayer3d(features[i], features[i-1],\n",
        "                    use_se=use_decoder_se,\n",
        "                    n_convs=decoder_n_convs[idx])\n",
        "            for idx, i in enumerate(reversed(range(2, len(features))))\n",
        "        ])\n",
        "\n",
        "        self.final_decoder_layer = DecoderLayer3d(features[1], features[0],\n",
        "                                    use_se=use_decoder_se,\n",
        "                                    n_convs=n_convs[0]) # Split from the rest for symmetry.\n",
        "\n",
        "        self.output = nn.Conv3d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, skip = self.initial_encoder_layer(x)\n",
        "        skip_connections = [skip]\n",
        "\n",
        "        for enc_layer in self.encoder:\n",
        "            x, skip = enc_layer(x)\n",
        "            skip_connections.append(skip)\n",
        "\n",
        "        skip_connections = skip_connections[:-1][::-1]\n",
        "\n",
        "        for dec_layer, skip in zip(self.decoder, skip_connections):\n",
        "            x = dec_layer(x, skip)\n",
        "\n",
        "        x = self.final_decoder_layer(x, skip_connections[-1])\n",
        "        return self.output(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLcIr7Bimm3h"
      },
      "source": [
        "## Loss Functions & Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:26:13.853587Z",
          "iopub.status.busy": "2026-01-28T20:26:13.853005Z",
          "iopub.status.idle": "2026-01-28T20:26:19.046233Z",
          "shell.execute_reply": "2026-01-28T20:26:19.045609Z",
          "shell.execute_reply.started": "2026-01-28T20:26:13.853556Z"
        },
        "id": "0kBIHSKjmtfz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def dice_score(\n",
        "    pred: torch.Tensor,\n",
        "    target: torch.Tensor,\n",
        "    eps: float = 1e-7,\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes per-sample Dice score for binary masks.\n",
        "\n",
        "    Args:\n",
        "        pred (Tensor): Prediction mask of shape (B, ...)\n",
        "        target (Tensor): Target mask of shape (B, ...)\n",
        "        eps (float): Numerical stability term\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Dice score per sample, shape (B,)\n",
        "    \"\"\"\n",
        "    assert pred.shape == target.shape\n",
        "\n",
        "    bs = pred.shape[0]\n",
        "    pred_flat = pred.reshape(bs, -1)\n",
        "    target_flat = target.reshape(bs, -1)\n",
        "\n",
        "    intersection = (pred_flat * target_flat).sum(dim=1)\n",
        "    union = pred_flat.sum(dim=1) + target_flat.sum(dim=1)\n",
        "\n",
        "    return (2.0 * intersection + eps) / (union + eps)\n",
        "\n",
        "def dice_loss(pred, target):\n",
        "    \"\"\"\n",
        "    Soft Dice loss for binary segmentation.\n",
        "    \"\"\"\n",
        "    return 1.0 - dice_score(pred, target).mean()\n",
        "\n",
        "def multiclass_soft_dice_loss(logits, target, ignore_idxs=None):\n",
        "    \"\"\"\n",
        "    Soft Dice loss averaged over classes.\n",
        "    \"\"\"\n",
        "    if ignore_idxs is None:\n",
        "        ignore_idxs = []\n",
        "\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    num_classes = probs.shape[1]\n",
        "\n",
        "    dice_losses = []\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        if c in ignore_idxs:\n",
        "            continue\n",
        "\n",
        "        target_c = (target == c).float()\n",
        "        dice_c = dice_score(probs[:, c], target_c)\n",
        "        dice_losses.append(1.0 - dice_c.mean())\n",
        "\n",
        "    if not dice_losses:  # Edge case, for ignore_idx == every idx, never gonna happen but still\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "\n",
        "    return sum(dice_losses) / len(dice_losses)\n",
        "\n",
        "def multiclass_soft_dice_ce_loss(\n",
        "    logits: torch.Tensor,\n",
        "    target: torch.Tensor,\n",
        "    ignore_idxs_dice: list[int] | None = None,\n",
        "    ignore_idxs_ce: list[int] | None = None,\n",
        "    alpha: float = 1.0,\n",
        "    beta: float = 0.0,\n",
        "    ce_class_weights: torch.Tensor | None = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes a combination of multi-class soft Dice loss and Cross-Entropy loss.\n",
        "\n",
        "    Args:\n",
        "        logits (Tensor): shape (B, C, H, W)\n",
        "        target (Tensor): shape (B, H, W)\n",
        "        ignore_idxs_dice (list[int] | None): classes to ignore for Dice loss\n",
        "        ignore_idxs_ce (list[int] | None): classes to ignore for CE loss\n",
        "        alpha (float): weight for Dice loss\n",
        "        beta (float): weight for CE loss\n",
        "        ce_class_weights (Tensor | None): optional class weights for CE loss\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Tensor, Tensor, Tensor]: total_loss, dice_loss, ce_loss\n",
        "    \"\"\"\n",
        "    if ignore_idxs_dice is None:\n",
        "        ignore_idxs_dice = []\n",
        "    if ignore_idxs_ce is None:\n",
        "        ignore_idxs_ce = []\n",
        "\n",
        "    dice = multiclass_soft_dice_loss(logits, target, ignore_idxs_dice)\n",
        "\n",
        "    if beta > 0.0:\n",
        "        n_classes = logits.shape[1]\n",
        "        # Default to unweighted if none provided\n",
        "        if ce_class_weights is None:\n",
        "            ce_class_weights = torch.ones(n_classes, device=logits.device)\n",
        "\n",
        "        # Zero out weights for ignored CE indices (ignore_idx parameter for CrossEntropyLoss by PyTorch doesn't support lists for ignore_idx parameter)\n",
        "        for idx in ignore_idxs_ce:\n",
        "            if 0 <= idx < n_classes:\n",
        "                ce_class_weights[idx] = 0.0\n",
        "\n",
        "        ce = nn.CrossEntropyLoss(weight=ce_class_weights)\n",
        "        ce_loss = ce(logits, target)\n",
        "    else:\n",
        "        ce_loss = torch.tensor(0.0, device=logits.device)\n",
        "\n",
        "    total = alpha * dice + beta * ce_loss\n",
        "    return total, dice, ce_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:26:19.04764Z",
          "iopub.status.busy": "2026-01-28T20:26:19.047387Z",
          "iopub.status.idle": "2026-01-28T20:26:19.05929Z",
          "shell.execute_reply": "2026-01-28T20:26:19.058514Z",
          "shell.execute_reply.started": "2026-01-28T20:26:19.04762Z"
        },
        "id": "v2seMzck9XST",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ComboLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper for multiclass_soft_dice_ce_loss to decouple Trainer from hyperparameters.\n",
        "\n",
        "    The Trainer remains 'blind' to specific parameters like alpha and beta, allowing it\n",
        "    to support any loss function following a standard interface. This prevents\n",
        "    hardcoding specific loss logic inside the training loop.\n",
        "    \"\"\"\n",
        "    def __init__(self, dice_weight=1.0, ce_weight=0.0, ignore_idxs_dice=None, ignore_idxs_ce=None, ce_class_weights=None):\n",
        "        super().__init__()\n",
        "        self.alpha = dice_weight\n",
        "        self.beta = ce_weight\n",
        "        self.ignore_idxs_dice = ignore_idxs_dice or []\n",
        "        self.ignore_idxs_ce = ignore_idxs_ce or []\n",
        "        if ce_class_weights is not None:\n",
        "            self.ce_class_weights = torch.tensor(ce_class_weights, dtype=torch.float32)\n",
        "        else:\n",
        "            self.ce_class_weights = None\n",
        "\n",
        "        if self.alpha > 0 and self.beta == 0:\n",
        "            self.loss_type = \"Dice Loss\"\n",
        "        elif self.alpha == 0 and self.beta > 0:\n",
        "            self.loss_type = \"Cross Entropy Loss\"\n",
        "        else:\n",
        "            self.loss_type = \"Combo Loss (Dice + CE)\"\n",
        "\n",
        "        print(f\"Loss configured: {self.loss_type} | Alpha: {self.alpha}, Beta: {self.beta}\")\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        ce_class_weights = self.ce_class_weights.to(logits.device) if self.ce_class_weights is not None else None\n",
        "        return multiclass_soft_dice_ce_loss(\n",
        "            logits,\n",
        "            target,\n",
        "            ignore_idxs_dice=self.ignore_idxs_dice,\n",
        "            ignore_idxs_ce=self.ignore_idxs_ce,\n",
        "            alpha=self.alpha,\n",
        "            beta=self.beta,\n",
        "            ce_class_weights=ce_class_weights\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:26:19.060516Z",
          "iopub.status.busy": "2026-01-28T20:26:19.060238Z",
          "iopub.status.idle": "2026-01-28T20:26:19.077409Z",
          "shell.execute_reply": "2026-01-28T20:26:19.076867Z",
          "shell.execute_reply.started": "2026-01-28T20:26:19.060488Z"
        },
        "id": "DGtoKVOYV3oU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def per_class_metrics(input, target, split='train'):\n",
        "    \"\"\"\n",
        "    Computes per-sample Dice, Jaccard, entropy and mean class probability\n",
        "    for each class, skipping ignored classes and samples without the class.\n",
        "    \"\"\"\n",
        "    C = input.shape[1] ## This is 'dangerous'\n",
        "    eps = 1e-7\n",
        "    hard_pred = input.argmax(dim=1)\n",
        "    per_class_metrics = {}\n",
        "\n",
        "    probs = F.softmax(input, dim=1)\n",
        "    entropy_map = -torch.sum(probs * torch.log(probs + eps), dim=1) # Computes Shannon entropy per-voxel\n",
        "    correct_class_probs = torch.gather(probs, 1, target.unsqueeze(1)).squeeze(1) # Models predicted probability for the true class at each voxel\n",
        "    #  torch.gather > take values from input along dimension dim at indices specified by index\n",
        "    # entropy_map and correct_class_probs complement each other very well in understanding the model's behavior.\n",
        "    for c in range(C):\n",
        "        bs = input.shape[0]\n",
        "        sample_mask = (target == c).view(bs, -1).sum(dim=1) > 0 # Flattens to (B, ...)\n",
        "        if sample_mask.sum() == 0: # If sample_mask is all False, the current class is absent in every sample of the batch -> Nothing to eval.\n",
        "            continue\n",
        "\n",
        "        target_c_samples = target[sample_mask] # Filter to valid samples only (where the class exists)\n",
        "        pred_c_samples = hard_pred[sample_mask]\n",
        "\n",
        "        target_mask = (target_c_samples == c)\n",
        "        pred_mask = (pred_c_samples == c)\n",
        "\n",
        "        # Dice per sample\n",
        "        dice = dice_score(pred_mask, target_mask, eps)\n",
        "        # Compute Jaccard from Dice\n",
        "        jaccard = dice / (2.0 - dice + eps)\n",
        "\n",
        "        per_class_metrics[f'labels_{split}/class_{c}_dice'] = dice.tolist()\n",
        "        per_class_metrics[f'labels_{split}/class_{c}_jaccard'] = jaccard.tolist()\n",
        "\n",
        "        # entropy & mean probability\n",
        "        entropy_c = entropy_map[sample_mask][target_mask]\n",
        "        prob_c = correct_class_probs[sample_mask][target_mask]\n",
        "\n",
        "        mean_entropy = entropy_c.mean()\n",
        "        mean_prob = prob_c.mean()\n",
        "\n",
        "        per_class_metrics[f'entropies_{split}/class_{c}'] = mean_entropy.item()\n",
        "        per_class_metrics[f'avg_prob_{split}/class_{c}'] = mean_prob.item()\n",
        "\n",
        "    return per_class_metrics\n",
        "\n",
        "def bra_ts_region_metrics(input, target, split='train'):\n",
        "    \"\"\"\n",
        "    Computes per-sample Dice and Jaccard for BraTS WT, TC, ET.\n",
        "    \"\"\"\n",
        "    pred = input.argmax(dim=1)\n",
        "    eps = 1e-7\n",
        "\n",
        "    # Define regions as binary masks\n",
        "    regions = {\n",
        "        'wt': ((pred == 1) | (pred == 2) | (pred == 3), (target == 1) | (target == 2) | (target == 3)),\n",
        "        'tc': ((pred == 1) | (pred == 3), (target == 1) | (target == 3)),\n",
        "        'et': (pred == 3, target == 3)\n",
        "    }\n",
        "\n",
        "    metrics = {}\n",
        "    for name, (p, t) in regions.items():\n",
        "        bs = t.shape[0]\n",
        "        sample_mask = t.view(bs, -1).sum(dim=1) > 0 # Flattens to (B, ...)\n",
        "        if sample_mask.sum() == 0: # If sample_mask is all False, the current region is absent in every sample of the batch -> Nothing to eval.\n",
        "            continue\n",
        "\n",
        "        p_masked = p[sample_mask] # Filter to valid samples only (where the region exists)\n",
        "        t_masked = t[sample_mask]\n",
        "\n",
        "        # Dice per sample\n",
        "        dice = dice_score(p_masked, t_masked, eps)\n",
        "        # Jaccard from Dice\n",
        "        jaccard = dice / (2.0 - dice + eps)\n",
        "\n",
        "        metrics[f'brats_{split}/{name}_dice'] = dice.tolist()\n",
        "        metrics[f'brats_{split}/{name}_jaccard'] = jaccard.tolist()\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rExBDSAcTENx"
      },
      "outputs": [],
      "source": [
        "class MetricAccumulator:\n",
        "    def __init__(self):\n",
        "        self.sums = defaultdict(float)\n",
        "        self.counts = defaultdict(int)\n",
        "\n",
        "    def add(self, metrics: dict):\n",
        "        for k, v in metrics.items():\n",
        "            if isinstance(v, list):\n",
        "                self.sums[k] += sum(v)\n",
        "                self.counts[k] += len(v)\n",
        "            else:\n",
        "                self.sums[k] += v\n",
        "                self.counts[k] += 1\n",
        "\n",
        "    def mean(self, key: str):\n",
        "        return self.sums[key] / self.counts[key]\n",
        "\n",
        "    def all_means(self):\n",
        "        return {k: self.mean(k) for k in self.sums}\n",
        "\n",
        "    def clear(self):\n",
        "        self.sums.clear()\n",
        "        self.counts.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey2XKc4WFs6o"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:26:19.079077Z",
          "iopub.status.busy": "2026-01-28T20:26:19.078485Z",
          "iopub.status.idle": "2026-01-28T20:26:19.103399Z",
          "shell.execute_reply": "2026-01-28T20:26:19.102704Z",
          "shell.execute_reply.started": "2026-01-28T20:26:19.079052Z"
        },
        "id": "BhuVQS83Ft4f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, scheduler, criterion, config, run,\n",
        "                 metrics_fn, brats_fn, device):\n",
        "        \"\"\"\n",
        "        Refactored Trainer with Optimized Interval Logging.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.criterion = criterion\n",
        "        self.config = config\n",
        "        self.run = run\n",
        "        self.device = device\n",
        "        self.acc_steps = self.config['training'].get('accumulation_steps', 1)\n",
        "        self.use_wandb = config['experiment'].get('wandb_logging', False)\n",
        "\n",
        "        self.metrics_fn = metrics_fn\n",
        "        self.brats_fn = brats_fn\n",
        "\n",
        "        self.best_dice_composite = 0.0\n",
        "        self.best_model_path = None\n",
        "\n",
        "    def train_epoch(self, dataloader, epoch, global_step):\n",
        "        self.model.train()\n",
        "        acc = MetricAccumulator()\n",
        "\n",
        "        step_times = []\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{self.config['training']['epochs']}\")\n",
        "\n",
        "        for i, (batch, labels) in enumerate(pbar):\n",
        "            step_start = time.time()\n",
        "\n",
        "            batch = batch.to(self.device)\n",
        "            labels = labels.to(self.device, dtype=torch.long)\n",
        "\n",
        "            outputs = self.model(batch)\n",
        "            loss, dice_comp, ce_comp = self.criterion(outputs, labels) # Loss is a single scalar\n",
        "\n",
        "            is_last_batch = (i + 1) == len(dataloader)\n",
        "            is_acc_step = (i + 1) % self.acc_steps == 0\n",
        "\n",
        "            (loss / self.acc_steps).backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pc_m = self.metrics_fn(outputs.detach(), labels, split='train')\n",
        "                br_m = self.brats_fn(outputs.detach(), labels, split='train')\n",
        "\n",
        "                acc.add({\n",
        "                    \"main/train_loss\": [loss.item()],\n",
        "                    \"loss_components/train_dice_loss\": [dice_comp.item()],\n",
        "                    \"loss_components/train_ce_loss\": [ce_comp.item()]\n",
        "                })\n",
        "\n",
        "                # Accumulate metrics\n",
        "                acc.add(pc_m)\n",
        "                acc.add(br_m)\n",
        "\n",
        "            if is_acc_step or is_last_batch: # Either step when it's time or when there are no more sample to process for this epoch.\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "                self.scheduler.step()\n",
        "\n",
        "                global_step += 1\n",
        "\n",
        "                log_dict = acc.all_means()\n",
        "                log_dict[\"loss_components/train_ce_dice_ratio\"] = (\n",
        "                    log_dict[\"loss_components/train_ce_loss\"] /\n",
        "                    (log_dict[\"loss_components/train_dice_loss\"] + 1e-8)\n",
        "                )\n",
        "                log_dict[\"main/lr\"] = self.optimizer.param_groups[0][\"lr\"]\n",
        "                log_dict[\"main/epoch\"] = epoch\n",
        "                self.log(log_dict, step=global_step, split=\"train\")\n",
        "\n",
        "                acc.clear()\n",
        "\n",
        "                if not self.use_wandb:\n",
        "                    pbar.set_postfix({\n",
        "                        \"Loss\": f\"{log_dict['main/train_loss']:.3f}\",\n",
        "                        \"WT\": f\"{log_dict.get('brats_train/wt_dice', 0.0):.3f}\",\n",
        "                        \"TC\": f\"{log_dict.get('brats_train/tc_dice', 0.0):.3f}\",\n",
        "                        \"ET\": f\"{log_dict.get('brats_train/et_dice', 0.0):.3f}\",\n",
        "                    })\n",
        "\n",
        "            step_times.append(time.time() - step_start)\n",
        "\n",
        "        self.log({\n",
        "            \"timing/epoch_train_sec\": time.time() - epoch_start,\n",
        "            \"timing/avg_step_sec\": sum(step_times) / len(step_times),\n",
        "        }, step=global_step, split=\"train\")\n",
        "\n",
        "        return global_step\n",
        "\n",
        "    def validate(self, dataloader, global_step, split='test'):\n",
        "        self.model.eval()\n",
        "        test_start = time.time()\n",
        "\n",
        "        acc = MetricAccumulator()\n",
        "        with torch.no_grad():\n",
        "            for tbatch, tlabels in tqdm(dataloader, leave=False, desc=f\"Validating {split}\"):\n",
        "                tbatch = tbatch.to(self.device)\n",
        "                tlabels = tlabels.to(self.device, dtype=torch.long)\n",
        "\n",
        "                tout = self.model(tbatch)\n",
        "                tloss, tdice, tce = self.criterion(tout, tlabels)\n",
        "\n",
        "                # Accumulate losses\n",
        "                acc.add({\n",
        "                    f\"main/{split}_loss\": [tloss.item()],\n",
        "                    f\"loss_components/{split}_dice_loss\": [tdice.item()],\n",
        "                    f\"loss_components/{split}_ce_loss\": [tce.item()]\n",
        "                })\n",
        "\n",
        "                # Accumulate metrics\n",
        "                pc_m = self.metrics_fn(tout, tlabels, split=split)\n",
        "                br_m = self.brats_fn(tout, tlabels, split=split)\n",
        "                acc.add(pc_m)\n",
        "                acc.add(br_m)\n",
        "\n",
        "        log_dict = acc.all_means()\n",
        "\n",
        "        # Composite Dice\n",
        "        comp_dice = (\n",
        "            log_dict.get(f'brats_{split}/wt_dice', 0.0) +\n",
        "            log_dict.get(f'brats_{split}/tc_dice', 0.0) +\n",
        "            log_dict.get(f'brats_{split}/et_dice', 0.0)\n",
        "        ) / 3\n",
        "\n",
        "        log_dict[f\"{split}/composite_dice\"] = comp_dice\n",
        "        log_dict[f\"timing/{split}_sec\"] = time.time() - test_start\n",
        "\n",
        "        self.log(log_dict, step=global_step, split=split)\n",
        "        acc.clear()\n",
        "        return comp_dice\n",
        "\n",
        "    def log(self, metrics_dict, step, split='train'):\n",
        "        \"\"\"Unified logging: WandB gets everything; Console only gets Validation.\"\"\"\n",
        "        if self.use_wandb and self.run is not None:\n",
        "            self.run.log(metrics_dict, step=step)\n",
        "\n",
        "        # When WandB is off, only print validation to keep terminal clean\n",
        "        elif split == 'test' and \"test/composite_dice\" in metrics_dict:\n",
        "            print(\n",
        "                f\"\\n### Validation [Step {step}] ###\\n\"\n",
        "                f\"Composite Dice: {metrics_dict['test/composite_dice']:.4f} | \"\n",
        "                f\"WT Dice: {metrics_dict.get('brats_test/wt_dice', 0.0):.4f} | \"\n",
        "                f\"TC Dice: {metrics_dict.get('brats_test/tc_dice', 0.0):.4f} | \"\n",
        "                f\"ET Dice: {metrics_dict.get('brats_test/et_dice', 0.0):.4f}\\n\"\n",
        "                f\"{'#' * 45}\"\n",
        "            )\n",
        "\n",
        "    def save_checkpoint(self, epoch, global_step, composite_dice, is_best=False):\n",
        "        \"\"\"\n",
        "        Saves a 'last_checkpoint' and keeps only the single best model.\n",
        "        \"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'global_step': global_step,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'config': self.config,\n",
        "        }\n",
        "\n",
        "        last_filename = \"./last_checkpoint.pth\"\n",
        "        torch.save(checkpoint, last_filename)\n",
        "\n",
        "        if is_best:\n",
        "            # Delete the previous best file if it exists\n",
        "            if self.best_model_path and os.path.exists(self.best_model_path):\n",
        "                try:\n",
        "                    os.remove(self.best_model_path)\n",
        "                except OSError as e:\n",
        "                    print(f\"Error deleting old best: {e}\")\n",
        "\n",
        "            new_best_name = f\"best_model_dice_{composite_dice:.4f}.pth\"\n",
        "            torch.save(checkpoint, new_best_name)\n",
        "\n",
        "            self.best_model_path = new_best_name\n",
        "            print(f\"New best model saved: {new_best_name}\")\n",
        "        else:\n",
        "            print(f\"Progress saved to {last_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpL8Y2sajyBZ"
      },
      "source": [
        "## Training\n",
        "\n",
        "When prompted, select option (2): Use an existing W&B account and paste your API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-28T20:26:19.104571Z",
          "iopub.status.busy": "2026-01-28T20:26:19.104274Z",
          "iopub.status.idle": "2026-01-29T04:08:26.119853Z",
          "shell.execute_reply": "2026-01-29T04:08:26.115594Z",
          "shell.execute_reply.started": "2026-01-28T20:26:19.104522Z"
        },
        "id": "6fTMMTBjE633",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = cvNet2(**config['model']).to(config['hardware']['device'])\n",
        "criterion = ComboLoss(dice_weight=config['loss']['dice_weight'], ce_weight=config['loss']['ce_weight'],\n",
        "                      ignore_idxs_dice=config['loss']['ignore_idxs_dice'], ignore_idxs_ce=config['loss']['ignore_idxs_ce'],\n",
        "                      ce_class_weights=config['loss']['ce_class_weights'])\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['training']['lr'],\n",
        "    weight_decay=config['training']['weight_decay']\n",
        ")\n",
        "\n",
        "total_steps = (len(train_dataloader) // config['training']['accumulation_steps']) * config['training']['epochs']\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=5e-5)\n",
        "\n",
        "run_name = (\n",
        "    f\"{type(model).__name__}_{type(criterion).__name__}_\"\n",
        "    f\"{config['loss']['dice_weight']}_{config['loss']['ce_weight']}_\"\n",
        "    f\"{type(optimizer).__name__}_{type(scheduler).__name__}_\"\n",
        "    f\"num_classes{config['model']['out_channels']}_\"\n",
        "    f\"bs{config['training']['train_batch_size']}_\"\n",
        "    f\"lr{config['training']['lr']}_{time.strftime('%d-%m-%Y_%H.%M')}\"\n",
        ")\n",
        "if config['experiment'].get('wandb_logging', False):\n",
        "    run = wandb.init(project=config['project'], name=run_name, config=config)\n",
        "else:\n",
        "    run = None\n",
        "    print(\"WandB logging is disabled. Standard console output will be used.\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    criterion=criterion,\n",
        "    config=config,\n",
        "    run=run,\n",
        "    metrics_fn=per_class_metrics,\n",
        "    brats_fn=bra_ts_region_metrics,\n",
        "    device=config['hardware']['device']\n",
        ")\n",
        "\n",
        "global_step = 0\n",
        "test_freq = config['training'].get('test_freq', 2)\n",
        "patience = config['training'].get('early_stopping_patience', 5)\n",
        "patience_counter = 0\n",
        "\n",
        "#current_dice = trainer.validate(test_dataloader, global_step, split='test')\n",
        "for epoch in range(1, config['training']['epochs'] + 1):\n",
        "    global_step = trainer.train_epoch(train_dataloader, epoch, global_step)\n",
        "\n",
        "    if epoch % test_freq == 0:\n",
        "        # Get the score from validation\n",
        "        current_dice = trainer.validate(test_dataloader, global_step, split='test')\n",
        "        is_best = current_dice > trainer.best_dice_composite\n",
        "\n",
        "        if is_best:\n",
        "            trainer.best_dice_composite = current_dice\n",
        "            if trainer.run is not None:\n",
        "                trainer.run.summary[\"best_val_dice\"] = current_dice\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"No improvement for {patience_counter} checks. (Best: {trainer.best_dice_composite:.4f})\")\n",
        "\n",
        "        trainer.save_checkpoint(\n",
        "            epoch=epoch,\n",
        "            global_step=global_step,\n",
        "            composite_dice=current_dice,\n",
        "            is_best=is_best\n",
        "        )\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "            break\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 1331968,
          "datasetId": 751906,
          "isSourceIdPinned": false,
          "sourceId": 1299795,
          "sourceType": "datasetVersion"
        },
        {
          "databundleVersionId": 15491303,
          "datasetId": 9359290,
          "sourceId": 14650962,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31260,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
