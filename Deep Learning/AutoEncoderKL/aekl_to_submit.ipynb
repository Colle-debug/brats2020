{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** Use the provided YAML config exactly as-is to replicate reported results."
      ],
      "metadata": {
        "id": "irYKt-KX_Fw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXNPnnOjPTpS"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U kagglehub lpips monai monai-generative torchio wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcIR9t6pPOlb"
      },
      "source": [
        "## Imports & Global configs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJbnsZKNYDeu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from typing import List, Optional\n",
        "\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchio as tio\n",
        "import wandb\n",
        "import yaml\n",
        "\n",
        "from generative.losses import PatchAdversarialLoss, PerceptualLoss\n",
        "from generative.networks.nets import AutoencoderKL, PatchDiscriminator\n",
        "from monai.networks.layers import Act\n",
        "from monai.utils import first, set_determinism\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "set_determinism(42)\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "config = load_config('/content/config_aekl.yaml')\n",
        "\n",
        "dataset_source = config[\"data\"][\"dataset_source\"]\n",
        "kaggle_preproccesed_dataset  = config[\"data\"][\"kaggle_preproccesed_dataset\"]\n",
        "preprocessed_path = None\n",
        "\n",
        "if dataset_source == \"original\":\n",
        "    print(\">>>Original mode<<<\\n\")\n",
        "    dataset_path = kagglehub.dataset_download(\n",
        "        \"awsaf49/brats20-dataset-training-validation\"\n",
        "    )\n",
        "    processed_path = config['experiment'].get(\"processed_path\", None) # Where to store processed files in case of store_locally=True\n",
        "    print(\"Original BraTS dataset downloaded.\")\n",
        "    print(\"Path to dataset files:\", dataset_path)\n",
        "    print(\"Path where processed/resampled .npy files will be stored [None means '<root_dir>/processed']:\", processed_path)\n",
        "\n",
        "elif dataset_source == \"preprocessed\":\n",
        "    print(\">>>Preprocessed mode<<<\\n\")\n",
        "    dataset_path = kagglehub.dataset_download(kaggle_preproccesed_dataset)\n",
        "    dataset_path = dataset_path + \"/content/\"\n",
        "    print(f\"{kaggle_preproccesed_dataset} dataset downloaded.\")\n",
        "    print(\"Path to dataset files:\", dataset_path)\n",
        "else:\n",
        "    raise ValueError(f\"Unknown dataset mode: {dataset_source}\")\n",
        "\n",
        "config['hardware']['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = config['hardware']['device']\n",
        "pprint(config, indent=4, width=80, compact=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c76gHqUHPKRz"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDMpYacOYFSa"
      },
      "outputs": [],
      "source": [
        "def _center_crop(volume: torch.Tensor, target_shape: tuple) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Center-crops a 3D volume to the target shape in (H, W, D) order.\n",
        "\n",
        "    Parameters:\n",
        "        volume (torch.Tensor): Input volume of shape (..., H, W, D).\n",
        "        target_shape (tuple): Target shape as (tH, tW, tD)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Cropped volume\n",
        "    \"\"\"\n",
        "    h, w, d = volume.shape[-3:]\n",
        "    tH, tW, tD = target_shape\n",
        "\n",
        "    h_start = (h - tH) // 2\n",
        "    w_start = (w - tW) // 2\n",
        "    d_start = (d - tD) // 2\n",
        "\n",
        "    return volume[...,\n",
        "                  h_start:h_start+tH,\n",
        "                  w_start:w_start+tW,\n",
        "                  d_start:d_start+tD]\n",
        "\n",
        "def apply_augmentation(\n",
        "    multimodal_tensor: torch.Tensor,\n",
        "    segmentation_tensor: torch.Tensor = None, # Made optional for Autoencoder use\n",
        "    modalities_list = ['t1', 't1ce', 't2', 'flair'],\n",
        "    geometric_transforms=None,\n",
        "    intensity_transforms=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Apply augmentation pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (transformed_multimodal, transformed_segmentation, foreground_mask)\n",
        "    \"\"\"\n",
        "\n",
        "    # Create TorchIO Subject\n",
        "    subject_dict = {}\n",
        "    for i, mod_name in enumerate(modalities_list):\n",
        "        subject_dict[mod_name] = tio.ScalarImage(tensor=multimodal_tensor[i:i+1]) # Slicing instead of indexing to preserver dim=0\n",
        "\n",
        "    # Only add segmentation to subject if it is provided\n",
        "    if segmentation_tensor is not None:\n",
        "        subject_dict['seg'] = tio.LabelMap(tensor=segmentation_tensor.unsqueeze(0))\n",
        "\n",
        "    subject = tio.Subject(**subject_dict)\n",
        "\n",
        "    # Apply geometric transforms\n",
        "    if geometric_transforms is not None:\n",
        "        subject = geometric_transforms(subject)\n",
        "\n",
        "    # Extract foreground mask after geometric transforms\n",
        "    # This is the important mask we will use for everything else.\n",
        "    mask = None\n",
        "    if intensity_transforms is not None:\n",
        "        # We can create the mask from any of the modalities, BraTS MRIS are co-registered.\n",
        "        for mod_name in modalities_list:\n",
        "            mask = subject[mod_name].data > 0\n",
        "            break\n",
        "\n",
        "    # Apply intensity transforms\n",
        "    if intensity_transforms is not None and mask is not None:\n",
        "        subject = intensity_transforms(subject)\n",
        "\n",
        "        # Reapply mask to zero out background\n",
        "        for mod_name in modalities_list:\n",
        "            # subject[mod_name].data = subject[mod_name].data * mask  [DeprecationWarning]\n",
        "            new_data = subject[mod_name].data * mask\n",
        "            subject[mod_name].set_data(new_data)\n",
        "\n",
        "    # Extract transformed tensors\n",
        "    multimodal_list = [subject[mod].data for mod in modalities_list]\n",
        "    transformed_multimodal = torch.cat(multimodal_list, dim=0)\n",
        "\n",
        "    # Handle the optional segmentation extraction\n",
        "    transformed_segmentation = None\n",
        "    if 'seg' in subject:\n",
        "        transformed_segmentation = subject['seg'].data.squeeze(0)\n",
        "\n",
        "    # Return the generated mask along with the tensors\n",
        "    # If no intensity transforms were applied, the mask will be None.\n",
        "    if mask is None:\n",
        "        mask = transformed_multimodal > 0\n",
        "\n",
        "    return transformed_multimodal, transformed_segmentation, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT0UdbiTWJQN"
      },
      "outputs": [],
      "source": [
        "def show_mri_slices(x: torch.Tensor, x_recon: torch.Tensor = None, slice_indices=None, cmap='gray', title=None):\n",
        "    \"\"\"\n",
        "    Visualize axial, coronal, and sagittal slices.\n",
        "    If x_recon is provided, shows ground truth (top) vs reconstruction (bottom).\n",
        "    If more than one modality is provided, first one is selected.\n",
        "    \"\"\"\n",
        "    def to_np(t):\n",
        "        t = t.squeeze()\n",
        "        return t[0].cpu().numpy() if t.ndim == 4 else t.cpu().numpy()\n",
        "\n",
        "    gt_np = to_np(x)\n",
        "    D, H, W = gt_np.shape\n",
        "    if slice_indices is None:\n",
        "        slice_indices = (D // 2, H // 2, W // 2)\n",
        "\n",
        "    rows = [gt_np]\n",
        "    if x_recon is not None:\n",
        "        rows.append(to_np(x_recon))\n",
        "\n",
        "    num_rows = len(rows)\n",
        "    fig, axes = plt.subplots(num_rows, 3, figsize=(12, 4 * num_rows))\n",
        "\n",
        "    if num_rows == 1:\n",
        "        axes = axes[None, :]\n",
        "\n",
        "    # Add the customized title if provided\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    titles = ['Axial', 'Coronal', 'Sagittal']\n",
        "    row_labels = ['GT', 'Recon'] if num_rows > 1 else ['']\n",
        "\n",
        "    for r_idx, data in enumerate(rows):\n",
        "        slices = [data[slice_indices[0], :, :], data[:, slice_indices[1], :], data[:, :, slice_indices[2]]]\n",
        "        for c_idx, (slc, t_sub) in enumerate(zip(slices, titles)):\n",
        "            ax = axes[r_idx, c_idx]\n",
        "            ax.imshow(slc, cmap=cmap)\n",
        "            if r_idx == 0:\n",
        "                ax.set_title(t_sub)\n",
        "            if c_idx == 0:\n",
        "                ax.set_ylabel(row_labels[r_idx])\n",
        "            ax.set_xticks([]); ax.set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk5ipISWQOaO"
      },
      "source": [
        "## Dataset Definition\n",
        "### Helpful infos\n",
        "\n",
        "The pipeline supports two data sources controlled by `dataset_source`:\n",
        "\n",
        "- **`\"original\"`**: Downloads the original BraTS dataset from Kaggle\n",
        "- **`\"preprocessed\"`**: Downloads a preprocessed Kaggle dataset (default: my 150×150×119 BraTS)\n",
        "\n",
        "The selected dataset becomes the `root_dir` for the `BraTSAutoEncoderDataset` instance.\n",
        "\n",
        "**Preprocessing & Caching:**\n",
        "- `processed_root_dir` can be explicitly set or defaults to `<root_dir>/processed`\n",
        "- If `store_locally=True` **and** `resample_shape` is provided:\n",
        "  - Data is resampled once and cached as `.npy` files in `processed_root_dir`\n",
        "  - Subsequent runs load from cache -> faster training\n",
        "- If `store_locally=False` **and** `resample_shape` is provided:\n",
        "  - Data is resampled on-the-fly every epoch (slower and [very] inefficient but saves disk space)\n",
        "- If `resample_shape=None`:\n",
        "  - Data is loaded as-is from source without resampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99OTTiKkYKF7"
      },
      "outputs": [],
      "source": [
        "class BraTSAutoEncoderDataset(Dataset):\n",
        "    \"\"\"\n",
        "    BraTS2020 Dataset for 3D multi-modal MRI brain tumor reconstruction.\n",
        "\n",
        "    Args:\n",
        "        split (str):\n",
        "            Dataset split to load. Must be either \"train\" (369 samples)\n",
        "            or \"test\" (125 samples). Augmentations are applied only when split=\"train\".\n",
        "\n",
        "        root_dir (Optional[str]):\n",
        "            Root directory containing the downloaded dataset.\n",
        "\n",
        "        processed_root_dir (Optional[str]):\n",
        "            Root directory used to store/read cached `.npy` files when store_locally=True.\n",
        "            Defaults to '<root_dir>/processed' if None.\n",
        "\n",
        "        modalities_list (List[str]):\n",
        "            List of MRI modalities to load (e.g., [\"t1ce\"]). Determines the number\n",
        "            of input channels.\n",
        "\n",
        "        resample_shape (Optional[tuple]):\n",
        "            Target spatial shape (D, H, W) for resampling. If None, volumes are\n",
        "            loaded at their original resolution.\n",
        "\n",
        "        store_locally (bool):\n",
        "            If True and `resample_shape` is specified, resampled volumes are saved\n",
        "            as `.npy` for faster future loading.\n",
        "\n",
        "        geometric_transforms:\n",
        "            TorchIO spatial transforms applied during training before cropping.\n",
        "\n",
        "        intensity_transforms:\n",
        "            TorchIO intensity transforms applied during training after cropping.\n",
        "\n",
        "        output_shape (Optional[tuple]):\n",
        "            Target spatial shape (D, H, W) for center cropping the final volume.\n",
        "            If None, the full (resampled) volume is returned.\n",
        "\n",
        "        norm_type (str):\n",
        "            Intensity normalization strategy applied channel-wise. Supported values:\n",
        "                - \"z\": Z-score normalization computed on the foreground.\n",
        "                - \"minmax\": Percentile-based min–max normalization to [0, 1].\n",
        "\n",
        "    Usage Modes:\n",
        "        1) Raw:\n",
        "            Load the volume as-is, from either NIfTI or existing `.npy`, without\n",
        "            resampling. Only optional cropping or augmentation is applied.\n",
        "\n",
        "        2) Raw + Resample:\n",
        "            Load the volume and resample on-the-fly to `resample_shape`. Source\n",
        "            can be NIfTI or `.npy`. No caching unless `store_locally=True`.\n",
        "\n",
        "        3) Raw + Resample + Store Locally:\n",
        "            Load the volume, resample to `resample_shape`, and store it as `.npy`\n",
        "            for faster subsequent access. Useful for training with fixed shapes\n",
        "            and high I/O efficiency.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: str = \"train\",\n",
        "        root_dir: str = \"../kaggle/input/brats20-dataset-training-validation/\",\n",
        "        processed_root_dir: Optional[str] = None,\n",
        "        modalities_list: List[str] = [\"t1ce\"],\n",
        "        resample_shape: Optional[tuple] = None,\n",
        "        store_locally: bool = False,\n",
        "        geometric_transforms=None,\n",
        "        intensity_transforms=None,\n",
        "        output_shape: Optional[tuple] = None,\n",
        "        norm_type: str = \"minmax\",\n",
        "        random_seed: int = 42,\n",
        "    ):\n",
        "\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.split = split\n",
        "        self.modalities_list = modalities_list\n",
        "        self.resample_shape = resample_shape\n",
        "        self.store_locally = store_locally\n",
        "        self.geometric_transforms = geometric_transforms\n",
        "        self.intensity_transforms = intensity_transforms\n",
        "        self.output_shape = output_shape\n",
        "        self.norm_type = norm_type\n",
        "\n",
        "        self.apply_augmentations = (\n",
        "            self.split == \"train\"\n",
        "            and (self.geometric_transforms is not None or self.intensity_transforms is not None)\n",
        "        )\n",
        "\n",
        "        if split == \"train\":\n",
        "            self.total_samples = 369\n",
        "            self.sample_name = \"Training\"\n",
        "        elif split == \"test\":\n",
        "            self.total_samples = 125\n",
        "            self.sample_name = \"Validation\"\n",
        "        else:\n",
        "            raise ValueError(\"split must be 'train' or 'test'\")\n",
        "        self.data_subdir = f\"BraTS2020_{self.sample_name}Data\"\n",
        "        self.data_subsubdir = f\"MICCAI_BraTS2020_{self.sample_name}Data\"\n",
        "\n",
        "        if root_dir is None:\n",
        "            raise ValueError(\"root_dir must be provided\")\n",
        "\n",
        "        self.processed_root = (Path(processed_root_dir) if processed_root_dir is not None else self.root_dir / \"processed\")\n",
        "\n",
        "        self.raw_samples_path = self.root_dir / self.data_subdir / self.data_subsubdir # This is the default file-structure for the released BraTS20 Dataset\n",
        "\n",
        "        if not self.raw_samples_path.exists():\n",
        "            raise FileNotFoundError(f\"Raw data not found at {self.raw_samples_path}\")\n",
        "\n",
        "        self.cached_samples_path = self.processed_root / self.data_subdir / self.data_subsubdir # We align the cached files to the original file-structure.\n",
        "\n",
        "        # If store_locally is True, perform preprocessing at instantiation\n",
        "        if self.store_locally and self.resample_shape is not None:\n",
        "            voxels_per_sample = np.prod(self.resample_shape)\n",
        "            bytes_per_sample = voxels_per_sample * len(self.modalities_list) * 4 # Assume fp32, this should be adaptive\n",
        "            total_gb = (bytes_per_sample * self.total_samples) / (1024**3)\n",
        "            print(f\"[INFO] Estimated storage: {total_gb:.2f} GB\")\n",
        "            self._preprocess_dataset()\n",
        "\n",
        "        self.use_resampling = self.resample_shape is not None\n",
        "\n",
        "    def _preprocess_dataset(self):\n",
        "        \"\"\"Preprocess and store all samples when store_locally=True.\"\"\"\n",
        "        self.cached_samples_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(f\"[INFO] Preprocessing dataset: {self.split}\")\n",
        "        for idx in tqdm(range(self.total_samples), desc=f\"Preprocessing {self.split}\", unit=\"sample\",):\n",
        "            sample_idx = f\"{idx + 1:03d}\"\n",
        "            sample_filename = f\"BraTS20_{self.sample_name}_{sample_idx}\"\n",
        "\n",
        "            if not self._cached_exists(sample_filename): # This is convenient, if you want to preprocess again, delete and restart runtime.\n",
        "                x = self._load_from_root(sample_filename)\n",
        "                x = self._resample(x)\n",
        "                self._save_npy(x, sample_filename)\n",
        "            else:\n",
        "                print(f\"[INFO] Sample {idx + 1}/{self.total_samples} already cached: {sample_filename}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_idx = f\"{idx + 1:03d}\"\n",
        "        sample_filename = f\"BraTS20_{self.sample_name}_{sample_idx}\"\n",
        "\n",
        "        # Load from processed storage\n",
        "        if self.store_locally and self.use_resampling:\n",
        "            x = self._load_npy(sample_filename)\n",
        "        else:\n",
        "            # Load from root_dir\n",
        "            x = self._load_from_root(sample_filename)\n",
        "\n",
        "            if self.use_resampling: # On-the-fly resampling: redundant but allows to train with no additional disk space utilized.\n",
        "                x = self._resample(x)\n",
        "\n",
        "        foreground_mask = None\n",
        "\n",
        "        if self.apply_augmentations:\n",
        "            x, _, foreground_mask = apply_augmentation(\n",
        "                x,\n",
        "                modalities_list=self.modalities_list,\n",
        "                geometric_transforms=self.geometric_transforms,\n",
        "                intensity_transforms=self.intensity_transforms,\n",
        "            )\n",
        "            if foreground_mask is not None:\n",
        "                foreground_mask = foreground_mask.squeeze(0)\n",
        "\n",
        "        if self.output_shape is not None:\n",
        "            x = _center_crop(x, self.output_shape)\n",
        "            if foreground_mask is not None:\n",
        "                foreground_mask = _center_crop(foreground_mask, self.output_shape)\n",
        "\n",
        "        if foreground_mask is None:\n",
        "            foreground_mask = x[0] > 0\n",
        "\n",
        "        x = self._normalize(x, foreground_mask)\n",
        "        return x\n",
        "\n",
        "    def _cached_exists(self, sample_filename: str) -> bool:\n",
        "        npy_file = (self.cached_samples_path / sample_filename / f\"{sample_filename}_resampled.npy\")\n",
        "        return npy_file.exists()\n",
        "\n",
        "    def _load_npy(self, sample_filename: str) -> torch.Tensor:\n",
        "        npy_file = (self.cached_samples_path / sample_filename / f\"{sample_filename}_resampled.npy\")\n",
        "        return torch.from_numpy(np.load(npy_file))\n",
        "\n",
        "    def _save_npy(self, tensor: torch.Tensor, sample_filename: str):\n",
        "        out_dir = self.cached_samples_path / sample_filename\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        npy_file = out_dir / f\"{sample_filename}_resampled.npy\"\n",
        "        np.save(npy_file, tensor.numpy())\n",
        "\n",
        "    def _load_from_root(self, sample_filename: str) -> torch.Tensor:\n",
        "        \"\"\"Load data from root_dir (either NIfTI or existing .npy).\"\"\"\n",
        "        # Check if there's an existing .npy file in root_dir\n",
        "        npy_file = self.raw_samples_path / sample_filename / f\"{sample_filename}_resampled.npy\"\n",
        "        if npy_file.exists():\n",
        "            return torch.from_numpy(np.load(npy_file))\n",
        "        else: # Fallback: .nii\n",
        "            return self._load_nifti(sample_filename)\n",
        "\n",
        "    def _load_nifti(self, sample_filename: str) -> torch.Tensor:\n",
        "        sample_path = self.raw_samples_path / sample_filename\n",
        "        volumes = []\n",
        "        for mod in self.modalities_list:\n",
        "            vol_path = sample_path / f\"{sample_filename}_{mod}.nii\"\n",
        "            volumes.append(self._load_nifti_volume(vol_path))\n",
        "        x = np.stack(volumes, axis=0)\n",
        "        return torch.from_numpy(x)\n",
        "\n",
        "    def _resample(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        subject = tio.Subject(image=tio.ScalarImage(tensor=x))\n",
        "        original_shape = subject.image.shape[-3:]\n",
        "        original_spacing = subject.image.spacing\n",
        "\n",
        "        target_shape = self.resample_shape\n",
        "        new_spacing = tuple(\n",
        "            o_sp * o_sh / t_sh\n",
        "            for o_sp, o_sh, t_sh in zip(original_spacing, original_shape, target_shape)\n",
        "        )\n",
        "\n",
        "        transform = tio.Compose(\n",
        "            [tio.ToCanonical(), tio.Resample(new_spacing, image_interpolation=\"bspline\")]\n",
        "        )\n",
        "\n",
        "        return transform(subject).image.data\n",
        "\n",
        "    def _normalize(self, tensor: torch.Tensor, foreground_mask: torch.Tensor) -> torch.Tensor:\n",
        "        out = torch.empty_like(tensor)\n",
        "\n",
        "        for c in range(tensor.shape[0]):\n",
        "            channel = tensor[c]\n",
        "\n",
        "            if self.norm_type == \"z\":\n",
        "                mask = foreground_mask\n",
        "                if mask.any():\n",
        "                    mean = channel[mask].mean()\n",
        "                    std = channel[mask].std()\n",
        "                    if std > 0:\n",
        "                        channel = channel.clone()\n",
        "                        channel[mask] = (channel[mask] - mean) / std\n",
        "                out[c] = channel\n",
        "\n",
        "            elif self.norm_type == \"minmax\":\n",
        "                subject = tio.Subject(\n",
        "                    image=tio.ScalarImage(tensor=channel.unsqueeze(0))\n",
        "                )\n",
        "                transform = tio.RescaleIntensity(\n",
        "                    out_min_max=(0, 1),\n",
        "                    percentiles=(0.0, 99.5),\n",
        "                )\n",
        "                out[c] = transform(subject).image.data[0]\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown normalization type: {self.norm_type}\")\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_nifti_volume(path: Path) -> np.ndarray:\n",
        "        nii = nib.load(str(path))\n",
        "        return np.asarray(nii.dataobj, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7owS9mXpAqKw"
      },
      "source": [
        "## Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhGtS23KbSa4"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(\n",
        "    train_batch_size=1,\n",
        "    test_batch_size=1,\n",
        "    geometric_transforms=None,\n",
        "    intensity_transforms=None,\n",
        "    output_shape=(112, 112, 96),\n",
        "    modalities_list=['t1ce'],\n",
        "    seed=42,\n",
        "    path='./content/',\n",
        "    processed_path=None,\n",
        "    num_workers=0,\n",
        "    norm_type='minmax',\n",
        "    resample_shape=(128, 128, 128),  # None means load as it is\n",
        "    store_locally=False  # Flag for caching resampled data, if resample_shape is None, this is not considered.\n",
        "):\n",
        "    \"\"\"\n",
        "    Create train and test dataloaders for BraTSAutoEncoderDataset.\n",
        "    \"\"\"\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(seed)\n",
        "\n",
        "    train_dataset = BraTSAutoEncoderDataset(\n",
        "        split='train',\n",
        "        root_dir=path,\n",
        "        processed_root_dir=processed_path,\n",
        "        modalities_list=modalities_list,\n",
        "        resample_shape=resample_shape,\n",
        "        store_locally=store_locally,\n",
        "        geometric_transforms=geometric_transforms,\n",
        "        intensity_transforms=intensity_transforms,\n",
        "        output_shape=output_shape,\n",
        "        norm_type=norm_type\n",
        "    )\n",
        "\n",
        "    test_dataset = BraTSAutoEncoderDataset(\n",
        "        split='test',\n",
        "        root_dir=path,\n",
        "        processed_root_dir=processed_path,\n",
        "        modalities_list=modalities_list,\n",
        "        resample_shape=resample_shape,\n",
        "        store_locally=store_locally,\n",
        "        geometric_transforms=None,  # no augmentation for test\n",
        "        intensity_transforms=None,\n",
        "        output_shape=output_shape,\n",
        "        norm_type=norm_type\n",
        "    )\n",
        "\n",
        "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=num_workers,\n",
        "        generator=g\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=test_batch_size,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return train_dataset, test_dataset, train_dataloader, test_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22We0MShxL0c"
      },
      "outputs": [],
      "source": [
        "geometric_transform=None\n",
        "intensity_transform=None\n",
        "\n",
        "data_cfg = config[\"data\"]\n",
        "training_cfg = config[\"training\"]\n",
        "\n",
        "train_dataset, test_dataset, train_dataloader, test_dataloader = get_dataloaders(\n",
        "    path=dataset_path,\n",
        "    geometric_transforms=geometric_transform,\n",
        "    intensity_transforms=intensity_transform,\n",
        "    train_batch_size=training_cfg[\"train_batch_size\"],\n",
        "    test_batch_size=training_cfg[\"test_batch_size\"],\n",
        "    output_shape=tuple(data_cfg[\"output_shape\"]),\n",
        "    modalities_list=data_cfg[\"modalities_list\"],\n",
        "    norm_type=data_cfg[\"norm_type\"],\n",
        "    processed_path=\"/content/processed\",\n",
        "    resample_shape=data_cfg[\"resample_shape\"],\n",
        "    store_locally=data_cfg[\"store_locally\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.utils import first\n",
        "batch = first(train_dataloader)\n",
        "print(\"Batch shape:\", batch.shape)\n",
        "show_mri_slices(batch)"
      ],
      "metadata": {
        "id": "H4uavbAify6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJP82-6Meo92"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjXJAD2zera5"
      },
      "outputs": [],
      "source": [
        "model = AutoencoderKL(\n",
        "    spatial_dims=config[\"model\"][\"spatial_dims\"],\n",
        "    in_channels=config[\"model\"][\"autoencoder\"][\"in_channels\"],\n",
        "    out_channels=config[\"model\"][\"autoencoder\"][\"out_channels\"],\n",
        "    num_channels=config[\"model\"][\"autoencoder\"][\"num_channels\"],\n",
        "    latent_channels=config[\"model\"][\"autoencoder\"][\"latent_channels\"],\n",
        "    num_res_blocks=config[\"model\"][\"autoencoder\"][\"num_res_blocks\"],\n",
        "    norm_num_groups=config[\"model\"][\"autoencoder\"][\"norm_num_groups\"],\n",
        "    attention_levels=config[\"model\"][\"autoencoder\"][\"attention_levels\"],\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "discriminator = PatchDiscriminator(\n",
        "    spatial_dims=config[\"model\"][\"spatial_dims\"],\n",
        "    num_layers_d=config[\"model\"][\"discriminator\"][\"num_layers_d\"],\n",
        "    num_channels=config[\"model\"][\"discriminator\"][\"num_channels\"],\n",
        "    in_channels=config[\"model\"][\"discriminator\"][\"in_channels\"],\n",
        "    out_channels=config[\"model\"][\"discriminator\"][\"out_channels\"],\n",
        "    kernel_size=config[\"model\"][\"discriminator\"][\"kernel_size\"],\n",
        "    padding=config[\"model\"][\"discriminator\"][\"padding\"], # Might remove this\n",
        "    activation=(Act.LEAKYRELU, {\"negative_slope\": 0.2}),\n",
        "    norm=\"BATCH\",\n",
        "    bias=False,\n",
        ")\n",
        "discriminator.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWp_4uCCuEmq"
      },
      "outputs": [],
      "source": [
        "# from torchinfo import summary\n",
        "# summary(model, input_size=(1, 1, 128, 128, 112))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYHs3Pgse6ip"
      },
      "source": [
        "## Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from generative.losses import PatchAdversarialLoss, PerceptualLoss\n",
        "loss_cfg = config[\"loss\"]\n",
        "recon_type = loss_cfg[\"reconstruction\"][\"type\"]\n",
        "\n",
        "if recon_type == \"l1\":\n",
        "    recon_loss_fn = F.l1_loss\n",
        "elif recon_type == \"l2\":\n",
        "    recon_loss_fn = F.mse_loss\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported reconstruction loss: {recon_type}\")\n",
        "\n",
        "perceptual_loss_fn = PerceptualLoss(\n",
        "    spatial_dims=loss_cfg[\"perceptual\"][\"spatial_dims\"],\n",
        "    network_type=loss_cfg[\"perceptual\"][\"network\"],\n",
        "    fake_3d_ratio=loss_cfg[\"perceptual\"][\"fake_3d_ratio\"],\n",
        ").to(device)\n",
        "\n",
        "adv_loss_fn = PatchAdversarialLoss(criterion=loss_cfg[\"adversarial\"][\"type\"])\n",
        "optimizer_g = torch.optim.Adam(model.parameters(), lr=float(config[\"training\"][\"lr_g\"]))\n",
        "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=float(config[\"training\"][\"lr_d\"]))\n",
        "\n",
        "scheduler_g = None\n",
        "scheduler_d = None"
      ],
      "metadata": {
        "id": "8vp69quhxCF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricAccumulator:\n",
        "    def __init__(self):\n",
        "        self.sums = defaultdict(float)\n",
        "        self.counts = defaultdict(int)\n",
        "\n",
        "    def add(self, metrics: dict):\n",
        "        for k, v in metrics.items():\n",
        "            if isinstance(v, list):\n",
        "                self.sums[k] += sum(v)\n",
        "                self.counts[k] += len(v)\n",
        "            else:\n",
        "                self.sums[k] += v\n",
        "                self.counts[k] += 1\n",
        "\n",
        "    def mean(self, key: str):\n",
        "        return self.sums[key] / self.counts[key]\n",
        "\n",
        "    def all_means(self):\n",
        "        return {k: self.mean(k) for k in self.sums}\n",
        "\n",
        "    def clear(self):\n",
        "        self.sums.clear()\n",
        "        self.counts.clear()"
      ],
      "metadata": {
        "id": "7uOn-GleiWDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz3cy1NPfkrQ"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmk5JIHhOt4P"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        discriminator,\n",
        "        optimizer_g,\n",
        "        optimizer_d,\n",
        "        scheduler_g,\n",
        "        scheduler_d,\n",
        "        recon_loss_fn,\n",
        "        perceptual_loss_fn,\n",
        "        adv_loss_fn,\n",
        "        config,\n",
        "        run,\n",
        "        device='cuda',\n",
        "        modalities_list=None,\n",
        "        checkpoint_dir=\"./checkpoints\"\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.discriminator = discriminator\n",
        "        self.optimizer_g = optimizer_g\n",
        "        self.optimizer_d = optimizer_d\n",
        "        self.scheduler_g = scheduler_g\n",
        "        self.scheduler_d = scheduler_d\n",
        "\n",
        "        self.recon_loss_fn = recon_loss_fn\n",
        "        self.perceptual_loss_fn = perceptual_loss_fn\n",
        "        self.adv_loss_fn = adv_loss_fn\n",
        "\n",
        "        self.config = config\n",
        "        self.run = run\n",
        "        self.device = device\n",
        "\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.modalities_list = config[\"data\"][\"modalities_list\"]\n",
        "        self.kl_weight = config[\"loss\"][\"kl_loss\"][\"weight\"]\n",
        "        self.adv_weight = config[\"loss\"][\"adversarial\"][\"gen_weight\"]\n",
        "        self.perceptual_weight = config[\"loss\"][\"perceptual\"][\"weight\"]\n",
        "        self.grad_accum_steps = config[\"training\"][\"accumulation_steps\"]\n",
        "        self.use_wandb = config[\"experiment\"][\"wandb_logging\"]\n",
        "\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_model_path = None\n",
        "\n",
        "    def train_epoch(self, dataloader, epoch, global_step):\n",
        "            self.model.train()\n",
        "            self.discriminator.train()\n",
        "\n",
        "            acc = MetricAccumulator()\n",
        "            step_times = []\n",
        "            epoch_start = time.time()\n",
        "\n",
        "            pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch}\")\n",
        "\n",
        "            for step, batch in pbar:\n",
        "                step_start = time.time()\n",
        "                images = batch.to(self.device)\n",
        "\n",
        "                reconstruction, z_mu, z_sigma = self.model(images)\n",
        "                logits_fake = self.discriminator(reconstruction.contiguous())[-1]\n",
        "\n",
        "                recons_loss = self.recon_loss_fn(reconstruction, images)\n",
        "                p_loss = self.perceptual_loss_fn(reconstruction, images)\n",
        "                gen_adv_loss = self.adv_loss_fn(logits_fake, target_is_real=True, for_discriminator=False)\n",
        "\n",
        "                kl_loss = 0.5 * torch.sum(z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, dim=[1, 2, 3, 4])\n",
        "                kl_loss = torch.mean(kl_loss)\n",
        "\n",
        "                loss_g = recons_loss + (self.kl_weight * kl_loss) + (self.perceptual_weight * p_loss) + (self.adv_weight * gen_adv_loss)\n",
        "\n",
        "                (loss_g / self.grad_accum_steps).backward()\n",
        "\n",
        "                logits_fake_d = self.discriminator(reconstruction.detach().contiguous())[-1]\n",
        "                loss_d_fake = self.adv_loss_fn(logits_fake_d, target_is_real=False, for_discriminator=True)\n",
        "\n",
        "                logits_real = self.discriminator(images.contiguous())[-1]\n",
        "                loss_d_real = self.adv_loss_fn(logits_real, target_is_real=True, for_discriminator=True)\n",
        "\n",
        "                discriminator_loss = (loss_d_fake + loss_d_real) * 0.5\n",
        "                loss_d = (self.adv_weight * discriminator_loss) / self.grad_accum_steps\n",
        "                loss_d.backward()\n",
        "\n",
        "                acc.add({\n",
        "                    \"train/recons_loss\":        recons_loss.item(),\n",
        "                    \"train/perceptual_loss\":    p_loss.item(),\n",
        "                    \"train/kl_loss\":            kl_loss.item(),\n",
        "                    \"train/generator_loss\":     gen_adv_loss.item(),\n",
        "                    \"train/discriminator_loss\": discriminator_loss.item(),\n",
        "                    \"train/total_gen_loss\":     loss_g.item(),\n",
        "                })\n",
        "\n",
        "                is_last_batch = (step + 1) == len(dataloader)\n",
        "                is_acc_step = (step + 1) % self.grad_accum_steps == 0\n",
        "\n",
        "                if is_acc_step or is_last_batch:\n",
        "                    self.optimizer_g.step()\n",
        "                    if self.scheduler_g is not None:\n",
        "                        self.scheduler_g.step()\n",
        "                    self.optimizer_g.zero_grad(set_to_none=True)\n",
        "\n",
        "                    self.optimizer_d.step()\n",
        "                    if self.scheduler_d is not None:\n",
        "                        self.scheduler_d.step()\n",
        "                    self.optimizer_d.zero_grad(set_to_none=True)\n",
        "\n",
        "                    global_step += 1\n",
        "\n",
        "                    log_dict = acc.all_means()\n",
        "                    log_dict.update({\n",
        "                        \"main/lr_g\": self.optimizer_g.param_groups[0][\"lr\"],\n",
        "                        \"main/lr_d\": self.optimizer_d.param_groups[0][\"lr\"],\n",
        "                        \"main/epoch\": epoch,\n",
        "                    })\n",
        "                    self.log(log_dict, step=global_step, split='train')\n",
        "                    acc.clear()\n",
        "\n",
        "                pbar.set_postfix({\"recon\": f\"{recons_loss.item():.3f}\", \"disc\": f\"{discriminator_loss.item():.3f}\"})\n",
        "                step_times.append(time.time() - step_start)\n",
        "\n",
        "            self.log({\n",
        "                \"timing/epoch_train_sec\": time.time() - epoch_start,\n",
        "                \"timing/avg_step_sec\": sum(step_times) / len(step_times),\n",
        "            }, step=global_step)\n",
        "\n",
        "            return global_step\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, dataloader, epoch, global_step):\n",
        "        self.model.eval()\n",
        "        val_start = time.time()\n",
        "\n",
        "        acc = MetricAccumulator()\n",
        "        pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Validation Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in pbar:\n",
        "            images = batch.to(self.device)\n",
        "            reconstruction, _, _ = self.model(images)\n",
        "\n",
        "            if step == 0:\n",
        "                if  self.run:\n",
        "                    self._log_visuals(images[0], reconstruction[0], step=global_step)\n",
        "                else:\n",
        "                    title=f\"Validation Epoch {epoch} | Step {global_step}\"\n",
        "                    show_mri_slices(images[0], reconstruction[0], title=title)\n",
        "\n",
        "            recons_loss = self.recon_loss_fn(reconstruction, images)\n",
        "\n",
        "            acc.add({\"validation/recons_loss\": recons_loss.item()})\n",
        "\n",
        "        log_dict = acc.all_means()\n",
        "        log_dict[\"timing/validation_sec\"] = time.time() - val_start\n",
        "\n",
        "        self.log(log_dict, step=global_step, split='val')\n",
        "\n",
        "        return log_dict[\"validation/recons_loss\"]\n",
        "\n",
        "    def log(self, metrics_dict, step, split='train'):\n",
        "        if self.use_wandb and self.run is not None:\n",
        "            self.run.log(metrics_dict, step=step)\n",
        "        elif split == 'val' and \"validation/recons_loss\" in metrics_dict:\n",
        "            summary = (\n",
        "                f\"\\n### Validation [Step {step}] ###\\n\"\n",
        "                f\"Recon Loss: {metrics_dict['validation/recons_loss']:.4f}\\n\"\n",
        "                f\"{'#' * 40}\"\n",
        "            )\n",
        "            tqdm.write(summary)\n",
        "\n",
        "    def _log_visuals(self, orig_tensor, recon_tensor, step):\n",
        "        orig = orig_tensor.cpu().numpy()\n",
        "        recon = recon_tensor.cpu().numpy()\n",
        "\n",
        "        C, H, W, D = orig.shape\n",
        "        center_h, center_w, center_d = H // 2, W // 2, D // 2\n",
        "        slice_caps = [\"axial\", \"coronal\", \"sagittal\"]\n",
        "\n",
        "        for m, mod in enumerate(self.modalities_list):\n",
        "            img, img_rec = orig[m], recon[m]\n",
        "\n",
        "            images_real = [\n",
        "                img[:, :, center_d],\n",
        "                img[:, center_w, :],\n",
        "                img[center_h, :, :]\n",
        "            ]\n",
        "            images_recon = [\n",
        "                img_rec[:, :, center_d],\n",
        "                img_rec[:, center_w, :],\n",
        "                img_rec[center_h, :, :]\n",
        "            ]\n",
        "\n",
        "            for slice_img, slice_rec, cap in zip(images_real, images_recon, slice_caps):\n",
        "                self.run.log({\n",
        "                    f\"validation_plot/{mod}/{cap}_real\": wandb.Image(slice_img),\n",
        "                    f\"validation_plot/{mod}/{cap}_recon\": wandb.Image(slice_rec),\n",
        "                    }, step=step)\n",
        "\n",
        "    def save_checkpoint(self, epoch, global_step, val_loss, is_best=False):\n",
        "        \"\"\"\n",
        "        Saves the model state. Keeps 'last.pth' and the single best model.\n",
        "        \"\"\"\n",
        "        state = {\n",
        "            'epoch': epoch,\n",
        "            'global_step': global_step,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': self.optimizer_g.state_dict(),\n",
        "            'optimizer_d_state_dict': self.optimizer_d.state_dict(),\n",
        "            'scheduler_g_state_dict': self.scheduler_g.state_dict() if self.scheduler_g else None,\n",
        "            'scheduler_d_state_dict': self.scheduler_d.state_dict() if self.scheduler_d else None,\n",
        "            'config': self.config,\n",
        "        }\n",
        "\n",
        "        last_path = self.checkpoint_dir / \"last_checkpoint.pth\"\n",
        "        torch.save(state, last_path)\n",
        "\n",
        "        if is_best:\n",
        "            # Delete previous best to save space (optional but recommended)\n",
        "            if self.best_model_path and os.path.exists(self.best_model_path):\n",
        "                try:\n",
        "                    os.remove(self.best_model_path)\n",
        "                except OSError:\n",
        "                    pass # Ignore if file missing\n",
        "\n",
        "            # Save new best\n",
        "            new_best_name = f\"best_model_loss_{val_loss:.4f}.pth\"\n",
        "            self.best_model_path = self.checkpoint_dir / new_best_name\n",
        "            torch.save(state, self.best_model_path)\n",
        "\n",
        "            print(f\"New best model saved: {new_best_name}\")\n",
        "        else:\n",
        "            print(f\"Checkpoint saved to {last_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "When prompted, select option (2): Use an existing W&B account and paste your API key."
      ],
      "metadata": {
        "id": "2iXQXxtajzpK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdByT7BjMGNH"
      },
      "outputs": [],
      "source": [
        "run = None\n",
        "def fmt(val): return f\"{val:.0e}\" if val < 0.001 else f\"{val}\"\n",
        "run_name = (\n",
        "    f\"aekl_\"\n",
        "    f\"KL{fmt(config['loss']['kl_loss']['weight'])}_\"\n",
        "    f\"Adv{fmt(config['loss']['adversarial']['gen_weight'])}_\"\n",
        "    f\"P{fmt(config['loss']['perceptual']['weight'])}_\"\n",
        "    f\"bs{config['training']['train_batch_size']}_\"\n",
        "    f\"LRg{fmt(config['training']['lr_g'])}_\"\n",
        "    f\"LRd{fmt(config['training']['lr_d'])}_\"\n",
        "    f\"{time.strftime('%d%b_%H-%M')}\"\n",
        ")\n",
        "if config['experiment'].get('wandb_logging', False):\n",
        "    print(\"WandB logging is enabled. Run is getting started...\")\n",
        "    run = wandb.init(project=config[\"project\"], name=run_name, config=config)\n",
        "else:\n",
        "    run = None\n",
        "    print(\"WandB logging is disabled. Standard console output will be used.\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    discriminator=discriminator,\n",
        "    optimizer_g=optimizer_g,\n",
        "    optimizer_d=optimizer_d,\n",
        "    scheduler_g=scheduler_g,\n",
        "    scheduler_d=scheduler_d,\n",
        "    recon_loss_fn=recon_loss_fn,\n",
        "    adv_loss_fn=adv_loss_fn,\n",
        "    perceptual_loss_fn=perceptual_loss_fn,\n",
        "    config=config,\n",
        "    run=run,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "global_step = 0\n",
        "patience_counter = 0\n",
        "test_freq = config['training'].get('val_interval', 2)\n",
        "patience = config['training'].get('early_stopping_patience', 5)\n",
        "\n",
        "#current_loss = trainer.validate(test_dataloader, epoch=0, global_step=0) # Testing purposes\n",
        "for epoch in range(1, config['training']['epochs'] + 1):\n",
        "\n",
        "    global_step = trainer.train_epoch(train_dataloader, epoch, global_step)\n",
        "\n",
        "    if (epoch + 1) % test_freq == 0:\n",
        "        current_loss = trainer.validate(test_dataloader, epoch, global_step)\n",
        "\n",
        "        is_best = current_loss < trainer.best_val_loss\n",
        "\n",
        "        if is_best:\n",
        "            trainer.best_val_loss = current_loss\n",
        "            if trainer.run:\n",
        "                trainer.run.summary[\"best_val_loss\"] = current_loss\n",
        "            # Reset counter if we found a better model\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"No improvement for {patience_counter} checks. (Best: {trainer.best_val_loss:.4f})\")\n",
        "\n",
        "        trainer.save_checkpoint(\n",
        "            epoch=epoch,\n",
        "            global_step=global_step,\n",
        "            val_loss=current_loss,\n",
        "            is_best=is_best\n",
        "        )\n",
        "\n",
        "        # Trigger early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "            break\n",
        "\n",
        "if run:\n",
        "    wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8935486,
          "sourceId": 14031963,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}