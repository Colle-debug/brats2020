{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zHiVyz_dF1S"
      },
      "source": [
        "# BraTS2020 Survival Prediction: Comprehensive Feature Analysis\n",
        "Binary classification of survival time (above/below median) using various feature engineering techniques combined with classical ML models.\n",
        "\n",
        "Dataset: BraTS2020 with pre-computed autoencoder embeddings (3×14×14×12 per patient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT6Oj9fedF1V"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "Import required libraries for data processing, machine learning algos and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30uPA1MSdF1W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    accuracy_score\n",
        ")\n",
        "\n",
        "# Configuration\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb9OOMWkdF1Z"
      },
      "source": [
        "## 2. Data Loading and Preparation\n",
        "\n",
        "Load the pre-processed embeddings and create binary survival labels based on median survival time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjT_SgRwdF1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9431fb6-a2ef-4973-ecdf-72226d0efebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 235 patients\n",
            "Embedding shape: (3, 14, 14, 12)\n",
            "Survival range: 5 - 1767 days\n",
            "Median survival: 370 days\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "data_path = \"./embeddings_aekl.pkl\"\n",
        "df = pd.read_pickle(data_path)\n",
        "\n",
        "print(f\"Dataset loaded: {len(df)} patients\")\n",
        "print(f\"Embedding shape: {df['embedding'].iloc[0].shape}\")\n",
        "print(f\"Survival range: {df['survival_days'].min():.0f} - {df['survival_days'].max():.0f} days\")\n",
        "print(f\"Median survival: {df['survival_days'].median():.0f} days\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "c3W9kDwwVMNz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "7a3e43a4-d7fc-4934-81b2-8a64bff3129b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              sample_id                                          embedding  \\\n",
              "0  BraTS20_Training_001  [[[[0.2374728  0.11662745 0.18001464 0.2038001...   \n",
              "1  BraTS20_Training_002  [[[[0.38166767 0.34106487 0.38072276 0.3726496...   \n",
              "2  BraTS20_Training_003  [[[[0.22839475 0.10543397 0.1688725  0.1925266...   \n",
              "3  BraTS20_Training_004  [[[[0.3078203  0.22849745 0.27818102 0.2623945...   \n",
              "4  BraTS20_Training_005  [[[[0.18848613 0.03515071 0.11100617 0.1380027...   \n",
              "\n",
              "      age  survival_days  \n",
              "0  60.463            289  \n",
              "1  52.263            616  \n",
              "2  54.301            464  \n",
              "3  39.068            788  \n",
              "4  68.493            465  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aed262f3-867b-4820-a143-c95324811370\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_id</th>\n",
              "      <th>embedding</th>\n",
              "      <th>age</th>\n",
              "      <th>survival_days</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BraTS20_Training_001</td>\n",
              "      <td>[[[[0.2374728  0.11662745 0.18001464 0.2038001...</td>\n",
              "      <td>60.463</td>\n",
              "      <td>289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BraTS20_Training_002</td>\n",
              "      <td>[[[[0.38166767 0.34106487 0.38072276 0.3726496...</td>\n",
              "      <td>52.263</td>\n",
              "      <td>616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BraTS20_Training_003</td>\n",
              "      <td>[[[[0.22839475 0.10543397 0.1688725  0.1925266...</td>\n",
              "      <td>54.301</td>\n",
              "      <td>464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BraTS20_Training_004</td>\n",
              "      <td>[[[[0.3078203  0.22849745 0.27818102 0.2623945...</td>\n",
              "      <td>39.068</td>\n",
              "      <td>788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BraTS20_Training_005</td>\n",
              "      <td>[[[[0.18848613 0.03515071 0.11100617 0.1380027...</td>\n",
              "      <td>68.493</td>\n",
              "      <td>465</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aed262f3-867b-4820-a143-c95324811370')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aed262f3-867b-4820-a143-c95324811370 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aed262f3-867b-4820-a143-c95324811370');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 235,\n  \"fields\": [\n    {\n      \"column\": \"sample_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 235,\n        \"samples\": [\n          \"BraTS20_Training_070\",\n          \"BraTS20_Training_252\",\n          \"BraTS20_Training_218\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.899429843904363,\n        \"min\": 18.975,\n        \"max\": 86.652,\n        \"num_unique_values\": 216,\n        \"samples\": [\n          57.345,\n          50.0,\n          33.888\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"survival_days\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 355,\n        \"min\": 5,\n        \"max\": 1767,\n        \"num_unique_values\": 217,\n        \"samples\": [\n          351,\n          437,\n          291\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e35M26UldF1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ad8919-9d36-4df5-e7ab-35a36608dfd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class distribution:\n",
            "Class 0 (<= 370 days): 118 patients (50.2%)\n",
            "Class 1 (> 370 days): 117 patients (49.8%)\n"
          ]
        }
      ],
      "source": [
        "# Create binary labels: 0 = short survival, 1 = long survival\n",
        "median_survival = df[\"survival_days\"].median()\n",
        "bins = [df[\"survival_days\"].min() - 1, median_survival, df[\"survival_days\"].max()]\n",
        "labels = [0, 1]\n",
        "\n",
        "df['survival_class'] = pd.cut(\n",
        "    df['survival_days'],\n",
        "    bins=bins,\n",
        "    labels=labels,\n",
        "    include_lowest=True\n",
        ")\n",
        "\n",
        "# Display class distribution\n",
        "class_counts = df['survival_class'].value_counts().sort_index()\n",
        "print(\"\\nClass distribution:\")\n",
        "print(f\"Class 0 (<= {median_survival:.0f} days): {class_counts[0]} patients ({class_counts[0]/len(df)*100:.1f}%)\")\n",
        "print(f\"Class 1 (> {median_survival:.0f} days): {class_counts[1]} patients ({class_counts[1]/len(df)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAd-D1HwdF1b"
      },
      "source": [
        "## 3. Feature Engineering\n",
        "\n",
        "Extract multiple feature representations from the (3x14x14x12) embeddings:\n",
        "\n",
        "1. Channel-wise statistics (9-dim): Mean, std, max for each channel\n",
        "2. Mean pooling (3-dim): Average across spatial dimensions\n",
        "3. Spatial projections (variable-dim): Mean pooling along different axes\n",
        "4. Flattened embeddings (7056-dim): Complete embedding as vector\n",
        "\n",
        "*Check Report.pdf for a more detailed explanation.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hFCHV8gdF1b"
      },
      "source": [
        "### 3.1 Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAcxY8ZpdF1b"
      },
      "outputs": [],
      "source": [
        "def compute_channel_statistics(embedding):\n",
        "    \"\"\"\n",
        "    Compute statistical features for each channel.\n",
        "\n",
        "    Returns: 9D feature vector [mean_ch0, std_ch0, max_ch0, ..., mean_ch2, std_ch2, max_ch2]\n",
        "    \"\"\"\n",
        "    statistics = []\n",
        "    for channel_idx in range(embedding.shape[0]):\n",
        "        channel_data = embedding[channel_idx]\n",
        "        statistics.extend([\n",
        "            np.mean(channel_data),\n",
        "            np.std(channel_data),\n",
        "            np.max(channel_data)\n",
        "        ])\n",
        "    return np.array(statistics)\n",
        "\n",
        "\n",
        "def compute_spatial_projection(embedding, axes):\n",
        "    \"\"\"\n",
        "    Compute mean pooling along specified spatial axes.\n",
        "\n",
        "    Args:\n",
        "        embedding: 4D array (C, H, W, D)\n",
        "        axes: tuple of axes to average over\n",
        "\n",
        "    Examples:\n",
        "        axes=(1, 2) -> Axial projection (average over height & width)\n",
        "        axes=(1, 3) -> Sagittal projection (average over height & depth)\n",
        "        axes=(2, 3) -> Coronal projection (average over width & depth)\n",
        "    \"\"\"\n",
        "    return embedding.mean(axis=axes).flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnepgEa3dF1d"
      },
      "source": [
        "### 3.2 Extract raw features (no Scaling/PCA yet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQaMd3V5dF1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b82e6c6-3540-4a66-d6cf-69195b59da6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic features extracted\n",
            "------------------------------------------------------------\n",
            "- Channel means: 1 feature each\n",
            "- Channel statistics: 9 features\n",
            "- Mean pooled: 3 features\n"
          ]
        }
      ],
      "source": [
        "# Individual channel means\n",
        "df['channel_0_mean'] = df['embedding'].apply(lambda x: np.mean(x[0]).item())\n",
        "df['channel_1_mean'] = df['embedding'].apply(lambda x: np.mean(x[1]).item())\n",
        "df['channel_2_mean'] = df['embedding'].apply(lambda x: np.mean(x[2]).item())\n",
        "\n",
        "# Channel statistics (9D)\n",
        "df['channel_statistics'] = df['embedding'].apply(compute_channel_statistics)\n",
        "\n",
        "# Mean pooled embedding (3D) (Equivalent to stacking channel_0_mean, channel_1_mean, channel_2_mean)\n",
        "df['mean_pooled'] = df['embedding'].apply(lambda x: np.mean(x, axis=(1, 2, 3)))\n",
        "\n",
        "print(\"Basic features extracted\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"- Channel means: 1 feature each\")\n",
        "print(f\"- Channel statistics: {df['channel_statistics'].iloc[0].shape[0]} features\")\n",
        "print(f\"- Mean pooled: {df['mean_pooled'].iloc[0].shape[0]} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiFe8c8ddF1e"
      },
      "source": [
        "### 3.3 Extract spatial projections (Raw, no PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP75kl6sdF1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe132da-6fd1-46bf-de55-6c52746e473c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting spatial projections (raw features):\n",
            "------------------------------------------------------------\n",
            "axial      (axes (1, 2)): shape (235, 36)\n",
            "sagittal   (axes (1, 3)): shape (235, 42)\n",
            "coronal    (axes (2, 3)): shape (235, 42)\n"
          ]
        }
      ],
      "source": [
        "# Define projection configurations\n",
        "projection_config = {\n",
        "    'axial': (1, 2),      # Average over height & width -> depth profile\n",
        "    'sagittal': (1, 3),   # Average over height & depth -> width profile\n",
        "    'coronal': (2, 3)     # Average over width & depth -> height profile\n",
        "}\n",
        "\n",
        "print(\"Extracting spatial projections (raw features):\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Store raw projections (will be PCA'd after split)\n",
        "raw_projections = {}\n",
        "for projection_name, axes in projection_config.items():\n",
        "    raw_proj = np.stack(\n",
        "        df['embedding'].apply(lambda x: compute_spatial_projection(x, axes)).values\n",
        "    )\n",
        "    raw_projections[projection_name] = raw_proj\n",
        "    print(f\"{projection_name:10s} (axes {axes}): shape {raw_proj.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1btXl-YdF1f"
      },
      "source": [
        "### 3.4 Extract flattened embeddings (Raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggD5QZhEdF1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9c22908-b3d1-4fda-fe0b-72f02cd0307f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened embedding shape: (235, 7056)\n"
          ]
        }
      ],
      "source": [
        "# Flatten entire embedding (3 × 14 × 14 × 12 = 7,056 dimensions)\n",
        "flattened_embeddings = np.stack(\n",
        "    df['embedding'].apply(lambda x: x.flatten()).values\n",
        ")\n",
        "\n",
        "print(f\"Flattened embedding shape: {flattened_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV1abXZCdF1f"
      },
      "source": [
        "### 3.5 Prepare Age feature (Raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEvv3sbDdF1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54980681-e3ed-48c5-b8aa-11ccb61edfb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age statistics:\n",
            "Mean: 61.2 years\n",
            "Range: 19 - 87 years\n"
          ]
        }
      ],
      "source": [
        "age_raw = df['age'].values.reshape(-1, 1)\n",
        "\n",
        "print(f\"Age statistics:\")\n",
        "print(f\"Mean: {df['age'].mean():.1f} years\")\n",
        "print(f\"Range: {df['age'].min():.0f} - {df['age'].max():.0f} years\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDx6UezhdF1g"
      },
      "source": [
        "## 4. Feature set definitions\n",
        "\n",
        "Define all raw feature combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dokBYAnSdF1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0a12b2-fc8c-472b-957e-5e963cb71e95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9 raw feature sets defined\n",
            "  - channel_0_mean: (235, 1)\n",
            "  - channel_1_mean: (235, 1)\n",
            "  - channel_2_mean: (235, 1)\n",
            "  - channel_statistics: (235, 9)\n",
            "  - mean_pooled: (235, 3)\n",
            "  - pca_axial: (235, 36) (PCA → 0.9)\n",
            "  - pca_sagittal: (235, 42) (PCA → 0.9)\n",
            "  - pca_coronal: (235, 42) (PCA → 0.9)\n",
            "  - pca_full_embedding: (235, 7056) (PCA → 10D)\n"
          ]
        }
      ],
      "source": [
        "# Define raw feature sets (no preprocessing applied yet)\n",
        "raw_feature_sets = {\n",
        "    # Basic channel features (already low-dimensional, no PCA needed)\n",
        "    'channel_0_mean': {\n",
        "        'data': df['channel_0_mean'].values.reshape(-1, 1),\n",
        "        'needs_pca': False,\n",
        "        'pca_components': None\n",
        "    },\n",
        "    'channel_1_mean': {\n",
        "        'data': df['channel_1_mean'].values.reshape(-1, 1),\n",
        "        'needs_pca': False,\n",
        "        'pca_components': None\n",
        "    },\n",
        "    'channel_2_mean': {\n",
        "        'data': df['channel_2_mean'].values.reshape(-1, 1),\n",
        "        'needs_pca': False,\n",
        "        'pca_components': None\n",
        "    },\n",
        "    'channel_statistics': {\n",
        "        'data': np.stack(df['channel_statistics'].values),\n",
        "        'needs_pca': False,\n",
        "        'pca_components': None\n",
        "    },\n",
        "    'mean_pooled': {\n",
        "        'data': np.stack(df['mean_pooled'].values).reshape(-1, 3),\n",
        "        'needs_pca': False,\n",
        "        'pca_components': None\n",
        "    },\n",
        "\n",
        "    # Spatial projections (will need PCA)\n",
        "    'pca_axial': {\n",
        "        'data': raw_projections['axial'],\n",
        "        'needs_pca': True,\n",
        "        'pca_components': 0.90 # Value selected heuristically.\n",
        "    },\n",
        "    'pca_sagittal': {\n",
        "        'data': raw_projections['sagittal'],\n",
        "        'needs_pca': True,\n",
        "        'pca_components': 0.90\n",
        "    },\n",
        "    'pca_coronal': {\n",
        "        'data': raw_projections['coronal'],\n",
        "        'needs_pca': True,\n",
        "        'pca_components': 0.90\n",
        "    },\n",
        "\n",
        "    # Full embedding\n",
        "    'pca_full_embedding': {\n",
        "        'data': flattened_embeddings,\n",
        "        'needs_pca': True,\n",
        "        'pca_components': 10 # Heuristics\n",
        "    },\n",
        "}\n",
        "\n",
        "print(f\"{len(raw_feature_sets)} raw feature sets defined\")\n",
        "for name, config in raw_feature_sets.items():\n",
        "    if config[\"needs_pca\"]:\n",
        "        if config[\"pca_components\"] > 1:\n",
        "            pca_info = f\" (PCA → {config['pca_components']}D)\"\n",
        "        else:\n",
        "            pca_info = f\" (PCA → {config['pca_components']})\"\n",
        "    else:\n",
        "        pca_info = \"\"\n",
        "\n",
        "    print(f\"  - {name}: {config['data'].shape}{pca_info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5A8tbUPdF1g"
      },
      "source": [
        "## 5. Model(s) configurations\n",
        "\n",
        "Define machine learning models and their hyperparameter grids for systematic evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUtvQd3ydF1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69a36a4-79bd-490d-c5f7-b8034716463d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 models configured:\n",
            "  - Logistic Regression\n",
            "  - Linear SVM\n",
            "  - RBF SVM\n",
            "  - Random Forest\n",
            "  - Gradient Boosting\n"
          ]
        }
      ],
      "source": [
        "# Cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Model configurations with hyperparameter grids\n",
        "model_configs = {\n",
        "    'Logistic Regression': (\n",
        "        Pipeline([('classifier', LogisticRegression(max_iter=5000, random_state=42))]),\n",
        "        {\n",
        "            'classifier__C': [0.01, 0.1, 1, 10],\n",
        "            'classifier__penalty': ['l2'],\n",
        "        }\n",
        "    ),\n",
        "\n",
        "    'Linear SVM': (\n",
        "        Pipeline([('classifier', SVC(kernel='linear', random_state=42))]),\n",
        "        {'classifier__C': [0.01, 0.1, 1, 10]}\n",
        "    ),\n",
        "\n",
        "    'RBF SVM': (\n",
        "        Pipeline([('classifier', SVC(kernel='rbf', random_state=42))]),\n",
        "        {\n",
        "            'classifier__C': [0.01, 0.1, 1, 10],\n",
        "            'classifier__gamma': ['scale', 0.001, 0.01]\n",
        "        }\n",
        "    ),\n",
        "\n",
        "    'Random Forest': (\n",
        "        Pipeline([('classifier', RandomForestClassifier(random_state=42))]),\n",
        "        {\n",
        "            'classifier__n_estimators': [100, 200, 500],\n",
        "            'classifier__max_depth': [None, 5, 10],\n",
        "            'classifier__min_samples_leaf': [1, 3, 5]\n",
        "        }\n",
        "    ),\n",
        "\n",
        "    'Gradient Boosting': (\n",
        "        Pipeline([('classifier', GradientBoostingClassifier(random_state=42))]),\n",
        "        {\n",
        "            'classifier__n_estimators': [100, 200],\n",
        "            'classifier__learning_rate': [0.05, 0.1],\n",
        "            'classifier__max_depth': [1, 2]\n",
        "        }\n",
        "    )\n",
        "}\n",
        "\n",
        "print(f\"{len(model_configs)} models configured:\")\n",
        "for model_name in model_configs.keys():\n",
        "    print(f\"  - {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONOek3V6dF1h"
      },
      "source": [
        "## 6. Helper Function for Preprocessing\n",
        "\n",
        "This function applies scaling and PCA ONLY on training data, then transforms test data using the fitted transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwm7B1mTdF1h"
      },
      "outputs": [],
      "source": [
        "def preprocess_features(X_train, X_test, needs_pca=False, n_components=None):\n",
        "    \"\"\"\n",
        "    Apply scaling and optionally PCA, fitted on training data.\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features (raw)\n",
        "        X_test: Test features (raw)\n",
        "        needs_pca: Whether to apply PCA\n",
        "        n_components: Number of PCA components (if needs_pca=True)\n",
        "\n",
        "    Returns:\n",
        "        X_train_processed, X_test_processed, variance_explained (or None if no PCA)\n",
        "    \"\"\"\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Apply PCA if needed\n",
        "    if needs_pca and n_components is not None:\n",
        "        pca = PCA(n_components=n_components, random_state=42)\n",
        "        X_train_processed = pca.fit_transform(X_train_scaled)\n",
        "        X_test_processed = pca.transform(X_test_scaled)\n",
        "        variance_explained = pca.explained_variance_ratio_.sum()\n",
        "        return X_train_processed, X_test_processed, variance_explained\n",
        "    else:\n",
        "        return X_train_scaled, X_test_scaled, None\n",
        "# Random Forest and Gradient Boosting do not require feature scaling,\n",
        "# but scaling features doesn't hurt and makes the code cleaner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ5mE7PSdF1h"
      },
      "source": [
        "## 7. Evaluation\n",
        "\n",
        "Evaluate all combinations of:\n",
        "- Feature sets (10 types)\n",
        "- With/without age (2 variants)\n",
        "- Machine learning models (5 types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgXRoMHTdF1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcfaca6-e03d-478d-a30b-8aad5c59add0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting evaluation: 95 total experiments\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Storage for all results\n",
        "all_results = []\n",
        "target_labels = df['survival_class'].values\n",
        "\n",
        "experiment_count = 0\n",
        "total_experiments = len(raw_feature_sets) * 2 * len(model_configs) + len(model_configs)\n",
        "\n",
        "print(f\"Starting evaluation: {total_experiments} total experiments\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0FMWB-ZdF1i"
      },
      "source": [
        "### 7.1 Baseline: Age-only Model\n",
        "*Note: Only the metrics for the best hyperparameter setting per model are printed.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7assAf3_dF1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b60cd8-26ed-4f53-d7d0-486623fb38b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[BASELINE] Age-only Models\n",
            "--------------------------------------------------------------------------------\n",
            "  [ 1/95] Logistic Regression  -> CV: 0.6163±0.1597 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [ 2/95] Linear SVM           -> CV: 0.6216±0.1432 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [ 3/95] RBF SVM              -> CV: 0.6259±0.1082 | Test F1: 0.5648 | Acc: 0.5745\n",
            "  [ 4/95] Random Forest        -> CV: 0.5666±0.0932 | Test F1: 0.5957 | Acc: 0.5957\n",
            "  [ 5/95] Gradient Boosting    -> CV: 0.6193±0.0926 | Test F1: 0.5648 | Acc: 0.5745\n",
            "\n",
            "Baseline evaluation complete\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n[BASELINE] Age-only Models\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "experiment_name = \"age_only\"\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    age_raw, target_labels,\n",
        "    train_size=0.80,\n",
        "    random_state=42,\n",
        "    stratify=target_labels\n",
        ")\n",
        "\n",
        "X_train, X_test, _ = preprocess_features(\n",
        "    X_train_raw, X_test_raw,\n",
        "    needs_pca=False\n",
        ")\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name, (pipeline, param_grid) in model_configs.items():\n",
        "    experiment_count += 1\n",
        "\n",
        "    # Grid search with cross-validation\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=cv_strategy,\n",
        "        scoring='f1_macro',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Test set predictions\n",
        "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "    # Compute all metrics\n",
        "    precision_per_class = precision_score(y_test, y_pred, average=None)\n",
        "    recall_per_class = recall_score(y_test, y_pred, average=None)\n",
        "    f1_per_class = f1_score(y_test, y_pred, average=None)\n",
        "\n",
        "    result = {\n",
        "        'experiment': experiment_name,\n",
        "        'features': 'age',\n",
        "        'includes_age': True,\n",
        "        'model': model_name,\n",
        "        'feature_dim': 1,\n",
        "        'variance_explained': None,\n",
        "\n",
        "        # Cross-validation performance\n",
        "        'cv_f1_mean': grid_search.best_score_,\n",
        "        'cv_f1_std': grid_search.cv_results_['std_test_score'][grid_search.best_index_],\n",
        "\n",
        "        # Test set metrics (macro-averaged)\n",
        "        'test_f1_macro': f1_score(y_test, y_pred, average='macro'),\n",
        "        'test_f1_weighted': f1_score(y_test, y_pred, average='weighted'),\n",
        "        'test_precision_macro': precision_score(y_test, y_pred, average='macro'),\n",
        "        'test_recall_macro': recall_score(y_test, y_pred, average='macro'),\n",
        "        'test_accuracy': accuracy_score(y_test, y_pred),\n",
        "\n",
        "        # Per-class metrics\n",
        "        'test_precision_class0': precision_per_class[0],\n",
        "        'test_recall_class0': recall_per_class[0],\n",
        "        'test_f1_class0': f1_per_class[0],\n",
        "        'test_precision_class1': precision_per_class[1],\n",
        "        'test_recall_class1': recall_per_class[1],\n",
        "        'test_f1_class1': f1_per_class[1],\n",
        "        'best_params': grid_search.best_params_,\n",
        "\n",
        "        # Best hyperparameters\n",
        "        'best_params': grid_search.best_params_\n",
        "    }\n",
        "\n",
        "    all_results.append(result)\n",
        "\n",
        "    print(f\"  [{experiment_count:2d}/{total_experiments}] {model_name:20s} -> \"\n",
        "          f\"CV: {result['cv_f1_mean']:.4f}±{result['cv_f1_std']:.4f} | \"\n",
        "          f\"Test F1: {result['test_f1_macro']:.4f} | \"\n",
        "          f\"Acc: {result['test_accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\nBaseline evaluation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y9VOL0SdF1i"
      },
      "source": [
        "### 7.2 Main Evaluation Loop\n",
        "\n",
        "*Note: Only the metrics for the best hyperparameter setting per model are printed.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znwLJBzHdF1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8765bef-9376-43ca-a0e7-fc0fa18ff45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "MAIN EVALUATION: Feature sets with/without Age\n",
            "================================================================================\n",
            "\n",
            "channel_0_mean - Raw shape: (235, 1)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 1)\n",
            "  [ 6/95] Logistic Regression  -> CV: 0.4399±0.0812 | Test F1: 0.5317 | Acc: 0.5319\n",
            "  [ 7/95] Linear SVM           -> CV: 0.3653±0.0503 | Test F1: 0.3188 | Acc: 0.4681\n",
            "  [ 8/95] RBF SVM              -> CV: 0.4891±0.0694 | Test F1: 0.4671 | Acc: 0.4681\n",
            "  [ 9/95] Random Forest        -> CV: 0.5292±0.0517 | Test F1: 0.4671 | Acc: 0.4681\n",
            "  [10/95] Gradient Boosting    -> CV: 0.5242±0.0320 | Test F1: 0.5106 | Acc: 0.5106\n",
            "\n",
            "channel_0_mean_with_age - Raw shape: (235, 1)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 2)\n",
            "  [11/95] Logistic Regression  -> CV: 0.6008±0.1616 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [12/95] Linear SVM           -> CV: 0.6009±0.1541 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [13/95] RBF SVM              -> CV: 0.6102±0.1341 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [14/95] Random Forest        -> CV: 0.5969±0.0764 | Test F1: 0.5071 | Acc: 0.5106\n",
            "  [15/95] Gradient Boosting    -> CV: 0.5972±0.1078 | Test F1: 0.5317 | Acc: 0.5319\n",
            "\n",
            "channel_1_mean - Raw shape: (235, 1)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 1)\n",
            "  [16/95] Logistic Regression  -> CV: 0.4453±0.0381 | Test F1: 0.5106 | Acc: 0.5106\n",
            "  [17/95] Linear SVM           -> CV: 0.3802±0.0354 | Test F1: 0.3505 | Acc: 0.4681\n",
            "  [18/95] RBF SVM              -> CV: 0.4576±0.0577 | Test F1: 0.5025 | Acc: 0.5106\n",
            "  [19/95] Random Forest        -> CV: 0.4572±0.0640 | Test F1: 0.5524 | Acc: 0.5532\n",
            "  [20/95] Gradient Boosting    -> CV: 0.5106±0.0890 | Test F1: 0.5025 | Acc: 0.5106\n",
            "\n",
            "channel_1_mean_with_age - Raw shape: (235, 1)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 2)\n",
            "  [21/95] Logistic Regression  -> CV: 0.5956±0.1557 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [22/95] Linear SVM           -> CV: 0.6063±0.1601 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [23/95] RBF SVM              -> CV: 0.6050±0.1269 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [24/95] Random Forest        -> CV: 0.5860±0.0440 | Test F1: 0.5266 | Acc: 0.5319\n",
            "  [25/95] Gradient Boosting    -> CV: 0.5913±0.0953 | Test F1: 0.5648 | Acc: 0.5745\n",
            "\n",
            "channel_2_mean - Raw shape: (235, 1)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 1)\n",
            "  [26/95] Logistic Regression  -> CV: 0.6067±0.0427 | Test F1: 0.5317 | Acc: 0.5319\n",
            "  [27/95] Linear SVM           -> CV: 0.5570±0.0354 | Test F1: 0.4835 | Acc: 0.4894\n",
            "  [28/95] RBF SVM              -> CV: 0.5930±0.0184 | Test F1: 0.5317 | Acc: 0.5319\n",
            "  [29/95] Random Forest        -> CV: 0.6099±0.0993 | Test F1: 0.5300 | Acc: 0.5319\n",
            "  [30/95] Gradient Boosting    -> CV: 0.6076±0.0764 | Test F1: 0.5071 | Acc: 0.5106\n",
            "\n",
            "channel_2_mean_with_age - Raw shape: (235, 1)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 2)\n",
            "  [31/95] Logistic Regression  -> CV: 0.6169±0.1522 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [32/95] Linear SVM           -> CV: 0.6219±0.1511 | Test F1: 0.5950 | Acc: 0.5957\n",
            "  [33/95] RBF SVM              -> CV: 0.6318±0.1365 | Test F1: 0.5727 | Acc: 0.5745\n",
            "  [34/95] Random Forest        -> CV: 0.6259±0.0789 | Test F1: 0.5696 | Acc: 0.5745\n",
            "  [35/95] Gradient Boosting    -> CV: 0.6125±0.1074 | Test F1: 0.5648 | Acc: 0.5745\n",
            "\n",
            "channel_statistics - Raw shape: (235, 9)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 9)\n",
            "  [36/95] Logistic Regression  -> CV: 0.5424±0.0492 | Test F1: 0.5106 | Acc: 0.5106\n",
            "  [37/95] Linear SVM           -> CV: 0.5551±0.0545 | Test F1: 0.4891 | Acc: 0.4894\n",
            "  [38/95] RBF SVM              -> CV: 0.5077±0.0649 | Test F1: 0.4593 | Acc: 0.4681\n",
            "  [39/95] Random Forest        -> CV: 0.5540±0.0545 | Test F1: 0.4778 | Acc: 0.4894\n",
            "  [40/95] Gradient Boosting    -> CV: 0.5648±0.1146 | Test F1: 0.5696 | Acc: 0.5745\n",
            "\n",
            "channel_statistics_with_age - Raw shape: (235, 9)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 10)\n",
            "  [41/95] Logistic Regression  -> CV: 0.6585±0.1188 | Test F1: 0.5524 | Acc: 0.5532\n",
            "  [42/95] Linear SVM           -> CV: 0.6376±0.1263 | Test F1: 0.5524 | Acc: 0.5532\n",
            "  [43/95] RBF SVM              -> CV: 0.5880±0.0933 | Test F1: 0.5317 | Acc: 0.5319\n",
            "  [44/95] Random Forest        -> CV: 0.6006±0.0422 | Test F1: 0.5499 | Acc: 0.5532\n",
            "  [45/95] Gradient Boosting    -> CV: 0.5997±0.1114 | Test F1: 0.5696 | Acc: 0.5745\n",
            "\n",
            "mean_pooled - Raw shape: (235, 3)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 3)\n",
            "  [46/95] Logistic Regression  -> CV: 0.5331±0.0564 | Test F1: 0.5098 | Acc: 0.5106\n",
            "  [47/95] Linear SVM           -> CV: 0.5273±0.0461 | Test F1: 0.5098 | Acc: 0.5106\n",
            "  [48/95] RBF SVM              -> CV: 0.5476±0.0160 | Test F1: 0.5499 | Acc: 0.5532\n",
            "  [49/95] Random Forest        -> CV: 0.5281±0.0929 | Test F1: 0.4466 | Acc: 0.4468\n",
            "  [50/95] Gradient Boosting    -> CV: 0.5560±0.0496 | Test F1: 0.5098 | Acc: 0.5106\n",
            "\n",
            "mean_pooled_with_age - Raw shape: (235, 3)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 4)\n",
            "  [51/95] Logistic Regression  -> CV: 0.6061±0.1390 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [52/95] Linear SVM           -> CV: 0.6060±0.1260 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [53/95] RBF SVM              -> CV: 0.6124±0.1253 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [54/95] Random Forest        -> CV: 0.6111±0.1002 | Test F1: 0.5266 | Acc: 0.5319\n",
            "  [55/95] Gradient Boosting    -> CV: 0.6038±0.0804 | Test F1: 0.5524 | Acc: 0.5532\n",
            "\n",
            "pca_axial - Raw shape: (235, 36)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 3) | Variance: 91.53%\n",
            "  [56/95] Logistic Regression  -> CV: 0.5439±0.0870 | Test F1: 0.5300 | Acc: 0.5319\n",
            "  [57/95] Linear SVM           -> CV: 0.5403±0.0989 | Test F1: 0.4835 | Acc: 0.4894\n",
            "  [58/95] RBF SVM              -> CV: 0.5657±0.0467 | Test F1: 0.4891 | Acc: 0.4894\n",
            "  [59/95] Random Forest        -> CV: 0.5029±0.0403 | Test F1: 0.4160 | Acc: 0.4255\n",
            "  [60/95] Gradient Boosting    -> CV: 0.5351±0.0664 | Test F1: 0.5317 | Acc: 0.5319\n",
            "\n",
            "pca_axial_with_age - Raw shape: (235, 36)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 4) | Variance: 91.53%\n",
            "  [61/95] Logistic Regression  -> CV: 0.6056±0.1410 | Test F1: 0.5317 | Acc: 0.5319\n",
            "  [62/95] Linear SVM           -> CV: 0.6166±0.1402 | Test F1: 0.5743 | Acc: 0.5745\n",
            "  [63/95] RBF SVM              -> CV: 0.6324±0.1312 | Test F1: 0.5300 | Acc: 0.5319\n",
            "  [64/95] Random Forest        -> CV: 0.5821±0.0897 | Test F1: 0.5648 | Acc: 0.5745\n",
            "  [65/95] Gradient Boosting    -> CV: 0.5670±0.1173 | Test F1: 0.6126 | Acc: 0.6170\n",
            "\n",
            "pca_sagittal - Raw shape: (235, 42)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 3) | Variance: 92.97%\n",
            "  [66/95] Logistic Regression  -> CV: 0.5370±0.0316 | Test F1: 0.4873 | Acc: 0.4894\n",
            "  [67/95] Linear SVM           -> CV: 0.5616±0.0570 | Test F1: 0.4778 | Acc: 0.4894\n",
            "  [68/95] RBF SVM              -> CV: 0.5534±0.0454 | Test F1: 0.5025 | Acc: 0.5106\n",
            "  [69/95] Random Forest        -> CV: 0.5297±0.0738 | Test F1: 0.5300 | Acc: 0.5319\n",
            "  [70/95] Gradient Boosting    -> CV: 0.5275±0.0742 | Test F1: 0.4671 | Acc: 0.4681\n",
            "\n",
            "pca_sagittal_with_age - Raw shape: (235, 42)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 4) | Variance: 92.97%\n",
            "  [71/95] Logistic Regression  -> CV: 0.5904±0.1366 | Test F1: 0.5532 | Acc: 0.5532\n",
            "  [72/95] Linear SVM           -> CV: 0.5954±0.1166 | Test F1: 0.5950 | Acc: 0.5957\n",
            "  [73/95] RBF SVM              -> CV: 0.5779±0.1038 | Test F1: 0.5499 | Acc: 0.5532\n",
            "  [74/95] Random Forest        -> CV: 0.5779±0.0771 | Test F1: 0.5300 | Acc: 0.5319\n",
            "  [75/95] Gradient Boosting    -> CV: 0.5676±0.0851 | Test F1: 0.5071 | Acc: 0.5106\n",
            "\n",
            "pca_coronal - Raw shape: (235, 42)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 3) | Variance: 90.79%\n",
            "  [76/95] Logistic Regression  -> CV: 0.5610±0.0273 | Test F1: 0.6083 | Acc: 0.6170\n",
            "  [77/95] Linear SVM           -> CV: 0.6004±0.0386 | Test F1: 0.6557 | Acc: 0.6596\n",
            "  [78/95] RBF SVM              -> CV: 0.5481±0.0391 | Test F1: 0.5317 | Acc: 0.5319\n",
            "  [79/95] Random Forest        -> CV: 0.5342±0.0606 | Test F1: 0.4343 | Acc: 0.4468\n",
            "  [80/95] Gradient Boosting    -> CV: 0.5442±0.0502 | Test F1: 0.5399 | Acc: 0.5532\n",
            "\n",
            "pca_coronal_with_age - Raw shape: (235, 42)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 4) | Variance: 90.79%\n",
            "  [81/95] Logistic Regression  -> CV: 0.6159±0.0891 | Test F1: 0.5499 | Acc: 0.5532\n",
            "  [82/95] Linear SVM           -> CV: 0.6273±0.1176 | Test F1: 0.5300 | Acc: 0.5319\n",
            "  [83/95] RBF SVM              -> CV: 0.5959±0.1282 | Test F1: 0.5727 | Acc: 0.5745\n",
            "  [84/95] Random Forest        -> CV: 0.6002±0.1032 | Test F1: 0.5696 | Acc: 0.5745\n",
            "  [85/95] Gradient Boosting    -> CV: 0.5744±0.1315 | Test F1: 0.5648 | Acc: 0.5745\n",
            "\n",
            "pca_full_embedding - Raw shape: (235, 7056)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 10) | Variance: 68.16%\n",
            "  [86/95] Logistic Regression  -> CV: 0.4915±0.0254 | Test F1: 0.5928 | Acc: 0.5957\n",
            "  [87/95] Linear SVM           -> CV: 0.5179±0.0449 | Test F1: 0.6323 | Acc: 0.6383\n",
            "  [88/95] RBF SVM              -> CV: 0.4823±0.0462 | Test F1: 0.5524 | Acc: 0.5532\n",
            "  [89/95] Random Forest        -> CV: 0.5489±0.0654 | Test F1: 0.5266 | Acc: 0.5319\n",
            "  [90/95] Gradient Boosting    -> CV: 0.4913±0.0322 | Test F1: 0.3830 | Acc: 0.3830\n",
            "\n",
            "pca_full_embedding_with_age - Raw shape: (235, 7056)\n",
            "  ----------------------------------------------------------------------------\n",
            "  Processed shape: (188, 11) | Variance: 68.16%\n",
            "  [91/95] Logistic Regression  -> CV: 0.5572±0.1256 | Test F1: 0.5532 | Acc: 0.5532\n",
            "  [92/95] Linear SVM           -> CV: 0.5434±0.0544 | Test F1: 0.5317 | Acc: 0.5319\n",
            "  [93/95] RBF SVM              -> CV: 0.4823±0.0462 | Test F1: 0.5524 | Acc: 0.5532\n",
            "  [94/95] Random Forest        -> CV: 0.5870±0.0618 | Test F1: 0.5098 | Acc: 0.5106\n",
            "  [95/95] Gradient Boosting    -> CV: 0.5813±0.0598 | Test F1: 0.5524 | Acc: 0.5532\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MAIN EVALUATION: Feature sets with/without Age\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for feature_name, feature_config in raw_feature_sets.items():\n",
        "    # Extract raw feature data\n",
        "    X_raw = feature_config['data']\n",
        "    needs_pca = feature_config['needs_pca']\n",
        "    n_components = feature_config['pca_components']\n",
        "\n",
        "    # Test both with and without age\n",
        "    for include_age in [False, True]:\n",
        "        experiment_name = f\"{feature_name}_with_age\" if include_age else feature_name\n",
        "\n",
        "        print(f\"\\n{experiment_name} - Raw shape: {X_raw.shape}\")\n",
        "        print(\"  \" + \"-\" * 76)\n",
        "\n",
        "        X_train_raw, X_test_raw, y_train, y_test, train_idx, test_idx = train_test_split(\n",
        "            X_raw,\n",
        "            target_labels,\n",
        "            np.arange(len(X_raw)),  # Will be used to retrieve ages\n",
        "            train_size=0.80,\n",
        "            random_state=42,\n",
        "            stratify=target_labels\n",
        "        )\n",
        "\n",
        "        X_train_processed, X_test_processed, var_explained = preprocess_features(\n",
        "            X_train_raw, X_test_raw,\n",
        "            needs_pca=needs_pca,\n",
        "            n_components=n_components\n",
        "        )\n",
        "\n",
        "        if include_age:\n",
        "            # Use the indices we already got from the split above\n",
        "            age_train_raw = age_raw[train_idx]\n",
        "            age_test_raw = age_raw[test_idx]\n",
        "\n",
        "            age_train_scaled, age_test_scaled, _ = preprocess_features(\n",
        "                age_train_raw, age_test_raw,\n",
        "                needs_pca=False\n",
        "            )\n",
        "\n",
        "            X_train = np.hstack([X_train_processed, age_train_scaled])\n",
        "            X_test = np.hstack([X_test_processed, age_test_scaled])\n",
        "        else:\n",
        "            X_train = X_train_processed\n",
        "            X_test = X_test_processed\n",
        "\n",
        "        final_dim = X_train.shape[1]\n",
        "        var_info = f\" | Variance: {var_explained:.2%}\" if var_explained else \"\"\n",
        "        print(f\"  Processed shape: {X_train.shape}{var_info}\")\n",
        "\n",
        "        # Evaluate each model\n",
        "        for model_name, (pipeline, param_grid) in model_configs.items():\n",
        "            experiment_count += 1\n",
        "\n",
        "            # Grid search with cross-validation\n",
        "            grid_search = GridSearchCV(\n",
        "                pipeline,\n",
        "                param_grid,\n",
        "                cv=cv_strategy,\n",
        "                scoring='f1_macro',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            grid_search.fit(X_train, y_train)\n",
        "\n",
        "            # Test set evaluation\n",
        "            y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "            # Compute all metrics\n",
        "            precision_per_class = precision_score(y_test, y_pred, average=None)\n",
        "            recall_per_class = recall_score(y_test, y_pred, average=None)\n",
        "            f1_per_class = f1_score(y_test, y_pred, average=None)\n",
        "\n",
        "            result = {\n",
        "                'experiment': experiment_name,\n",
        "                'features': feature_name,\n",
        "                'includes_age': include_age,\n",
        "                'model': model_name,\n",
        "                'feature_dim': final_dim,\n",
        "                'variance_explained': var_explained,\n",
        "\n",
        "                # Cross-validation performance\n",
        "                'cv_f1_mean': grid_search.best_score_,\n",
        "                'cv_f1_std': grid_search.cv_results_['std_test_score'][grid_search.best_index_],\n",
        "\n",
        "                # Test set metrics (macro-averaged)\n",
        "                'test_f1_macro': f1_score(y_test, y_pred, average='macro'),\n",
        "                'test_f1_weighted': f1_score(y_test, y_pred, average='weighted'),\n",
        "                'test_precision_macro': precision_score(y_test, y_pred, average='macro'),\n",
        "                'test_recall_macro': recall_score(y_test, y_pred, average='macro'),\n",
        "                'test_accuracy': accuracy_score(y_test, y_pred),\n",
        "\n",
        "                # Per-class metrics\n",
        "                'test_precision_class0': precision_per_class[0],\n",
        "                'test_recall_class0': recall_per_class[0],\n",
        "                'test_f1_class0': f1_per_class[0],\n",
        "                'test_precision_class1': precision_per_class[1],\n",
        "                'test_recall_class1': recall_per_class[1],\n",
        "                'test_f1_class1': f1_per_class[1],\n",
        "\n",
        "                # Best hyperparameters\n",
        "                'best_params': grid_search.best_params_\n",
        "            }\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "            # Progress update\n",
        "            print(f\"  [{experiment_count:2d}/{total_experiments}] {model_name:20s} -> \"\n",
        "                  f\"CV: {result['cv_f1_mean']:.4f}±{result['cv_f1_std']:.4f} | \"\n",
        "                  f\"Test F1: {result['test_f1_macro']:.4f} | \"\n",
        "                  f\"Acc: {result['test_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4qBL5TEdF1j"
      },
      "source": [
        "## 8. Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVNJz5AsdF1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6792f5e-ef5f-4917-c6cc-453d2d9f0a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total experiments conducted: 95\n",
            "\n",
            "Best performing configuration:\n",
            "  Experiment: pca_coronal\n",
            "  Model: Linear SVM\n",
            "  Test F1: 0.6557\n",
            "  Test Accuracy: 0.6596\n"
          ]
        }
      ],
      "source": [
        "# Convert to DataFrame and sort by test F1 score\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_df = results_df.sort_values(by='test_f1_macro', ascending=False)\n",
        "\n",
        "print(f\"Total experiments conducted: {len(results_df)}\")\n",
        "print(f\"\\nBest performing configuration:\")\n",
        "best = results_df.iloc[0]\n",
        "print(f\"  Experiment: {best['experiment']}\")\n",
        "print(f\"  Model: {best['model']}\")\n",
        "print(f\"  Test F1: {best['test_f1_macro']:.4f}\")\n",
        "print(f\"  Test Accuracy: {best['test_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgBX-tEsdF1j"
      },
      "source": [
        "### 8.1 Top 10 Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZQ8dKZwdF1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02619646-035b-45d6-b340-3bf1e612ff6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "TOP 10 CONFIGURATIONS BY TEST F1 SCORE\n",
            "====================================================================================================\n",
            "             experiment               model  test_f1_macro  test_precision_macro  test_recall_macro  test_accuracy\n",
            "            pca_coronal          Linear SVM       0.655678              0.671456           0.662138       0.659574\n",
            "     pca_full_embedding          Linear SVM       0.632306              0.652941           0.641304       0.638298\n",
            "     pca_axial_with_age   Gradient Boosting       0.612637              0.626437           0.619565       0.617021\n",
            "            pca_coronal Logistic Regression       0.608333              0.634073           0.620471       0.617021\n",
            "               age_only       Random Forest       0.595745              0.596014           0.596014       0.595745\n",
            "  pca_sagittal_with_age          Linear SVM       0.595011              0.597985           0.596920       0.595745\n",
            "channel_2_mean_with_age          Linear SVM       0.595011              0.597985           0.596920       0.595745\n",
            "     pca_full_embedding Logistic Regression       0.592795              0.601504           0.597826       0.595745\n",
            "channel_2_mean_with_age Logistic Regression       0.574275              0.575455           0.575181       0.574468\n",
            "               age_only          Linear SVM       0.574275              0.575455           0.575181       0.574468\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Display top 10 results\n",
        "top_10 = results_df.head(10)[[\n",
        "    'experiment',\n",
        "    'model',\n",
        "    'test_f1_macro',\n",
        "    'test_precision_macro',\n",
        "    'test_recall_macro',\n",
        "    'test_accuracy'\n",
        "]]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"TOP 10 CONFIGURATIONS BY TEST F1 SCORE\")\n",
        "print(\"=\" * 100)\n",
        "print(top_10.to_string(index=False))\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGbfEGikdF1l"
      },
      "source": [
        "## 9. Export Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VW2Cfc3dF1m"
      },
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_filename = f\"brats2020_results_{timestamp}.csv\"\n",
        "results_df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\nResults exported to: {output_filename}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}